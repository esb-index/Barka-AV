{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyML/Ae7ZRpbnIdN3gby0lM4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/esb-index/Barka-AV/blob/main/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === CPRI annual preproc + MICE imputations + MC propagation ===\n",
        "# Colab/Local: mÃ¡sold be Ã©s futtasd. FeltÃ©telezzÃ¼k, hogy:\n",
        "# - van egy mappa (pl. /content/era5_processed_light) amiben vannak: dania_daily_light.csv, nemet_daily_light.csv, uk_daily_light.csv, tajvan_daily_light.csv, usa_daily_light.csv\n",
        "# - van assets.csv (asset_id, name, type, lat, lon, capacity, commission_year, ...)\n",
        "# - opcionÃ¡lisan exposure_matrix.csv Ã©s vulnerability_params.json\n",
        "#\n",
        "# Kimenetek:\n",
        "# hazard_yearly_<hazard>.csv\n",
        "# exposure_matrix.csv (ha generÃ¡ljuk vagy hasznÃ¡ljuk)\n",
        "# vulnerability_matrix.csv\n",
        "# r_values.csv (MC eredmÃ©nyek)\n",
        "# processing_log.json\n",
        "\n",
        "!pip install pandas numpy scipy scikit-learn openpyxl tqdm\n",
        "\n",
        "import os, glob, json, math\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from sklearn.experimental import enable_iterative_imputer  # fontos: ezt a sort kell elÅ‘bb\n",
        "from sklearn.impute import IterativeImputer\n",
        "from sklearn.linear_model import BayesianRidge\n",
        "from scipy import stats\n",
        "from datetime import datetime\n",
        "\n",
        "# ---------- CONFIG ----------\n",
        "WORKDIR = \"/content/era5_processed_light\"  # itt talÃ¡lhatÃ³k a daily CSV-k\n",
        "ASSETS_CSV = \"/content/assets.csv\"         # ha nincs, a script lÃ©trehoz default asset listÃ¡t\n",
        "EXPOSURE_CSV = \"/content/exposure_matrix.csv\"   # ha lÃ©tezik, beolvaszuk\n",
        "VULN_PARAMS_JSON = \"/content/vulnerability_params.json\"  # ha nincs, beÃ¡llÃ­tunk alapokat\n",
        "\n",
        "OUTPUT_DIR = \"/content/cpri_outputs\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "REFERENCE_START = 2000\n",
        "REFERENCE_END = 2022\n",
        "WINSOR_LO = 1    # %\n",
        "WINSOR_HI = 99   # %\n",
        "M_IMPUTE = 5     # MICE imputÃ¡ciÃ³k szÃ¡ma (MonteCarlo)\n",
        "MC_SEED = 42\n",
        "\n",
        "# Hazard mapping -> vÃ¡ltozÃ³nevek a daily csv-ben (a ti csv-k az elÅ‘zÅ‘ lÃ©pÃ©sbÅ‘l)\n",
        "HAZARD_MAP = {\n",
        "    \"heatwave\": {\n",
        "        \"var\": \"t2m_mean\",\n",
        "        \"intensity_func\": lambda arr: np.nanpercentile(arr, 99),   # H_raw = p99\n",
        "        \"freq_threshold_func\": lambda df: df[\"t2m_mean\"] > df[\"t2m_mean\"].quantile(0.95)  # P_raw: days/year > 95pct daily baseline\n",
        "    },\n",
        "    \"flood\": {\n",
        "        \"var\": \"tp_sum\",\n",
        "        \"intensity_func\": lambda arr: np.nanpercentile(arr, 95),\n",
        "        \"freq_threshold_func\": lambda df: df[\"tp_sum\"] >= df[\"tp_sum\"].quantile(0.90)\n",
        "    },\n",
        "    \"windstorm\": {\n",
        "        \"var\": None, # we'll check multiple: daily_max_gust or wind100_max\n",
        "        \"candidates\": [\"10fg_max\",\"wind100_max\",\"wind10_max\",\"daily_max_gust\"],\n",
        "        \"intensity_func\": lambda arr: np.nanpercentile(arr, 99),\n",
        "        \"freq_threshold_func\": lambda df: (df.filter(regex=\"10fg|gust|wind100|max\").max(axis=1) > df.filter(regex=\"10fg|gust|wind100|max\").quantile(0.95))\n",
        "    }\n",
        "    # Add more hazards if needed...\n",
        "}\n",
        "\n",
        "# ---------- helper functions ----------\n",
        "def read_daily_csvs(workdir):\n",
        "    files = sorted(glob.glob(os.path.join(workdir, \"*_daily_light.csv\")))\n",
        "    datasets = {}\n",
        "    for f in files:\n",
        "        key = os.path.basename(f).split(\"_\")[0]  # dania, nemet, uk, tajvan, usa\n",
        "        df = pd.read_csv(f, index_col=0, parse_dates=True)\n",
        "        datasets[key] = df\n",
        "    return datasets\n",
        "\n",
        "def ensure_assets():\n",
        "    if os.path.exists(ASSETS_CSV):\n",
        "        assets = pd.read_csv(ASSETS_CSV, dtype={\"asset_id\":str})\n",
        "    else:\n",
        "        # create placeholder assets: assume asset per region (user will replace with real assets)\n",
        "        rows = []\n",
        "        for i, r in enumerate([\"dania\",\"nemet\",\"uk\",\"tajvan\",\"usa\"], start=1):\n",
        "            rows.append({\"asset_id\":f\"{r}_001\",\"name\":f\"{r}_avg\",\"type\":\"OffshoreWind\",\"lat\":np.nan,\"lon\":np.nan})\n",
        "        assets = pd.DataFrame(rows)\n",
        "        assets.to_csv(ASSETS_CSV, index=False)\n",
        "    return assets\n",
        "\n",
        "def load_exposure_matrix():\n",
        "    if os.path.exists(EXPOSURE_CSV):\n",
        "        return pd.read_csv(EXPOSURE_CSV, dtype={\"asset_id\":str,\"hazard_id\":str})\n",
        "    else:\n",
        "        # If no exposure supplied, we create a default E=1 for all asset-hazard combinations (user should refine later)\n",
        "        return None\n",
        "\n",
        "def load_vuln_params():\n",
        "    if os.path.exists(VULN_PARAMS_JSON):\n",
        "        with open(VULN_PARAMS_JSON,\"r\") as fh:\n",
        "            return json.load(fh)\n",
        "    else:\n",
        "        # default baseline V0 per type\n",
        "        return {\n",
        "            \"defaults\": {\n",
        "                \"OffshoreWind\": 0.4,\n",
        "                \"OnshoreWind\": 0.35,\n",
        "                \"PV\": 0.3,\n",
        "                \"Biomass\": 0.6,\n",
        "                \"Gas\": 0.7\n",
        "            },\n",
        "            # hyperparams for formula (can be tuned)\n",
        "            \"age_scale_factor\": 0.5,\n",
        "            \"redundancy_factor\": 0.3,\n",
        "            \"maintenance_factor\": 0.4\n",
        "        }\n",
        "\n",
        "def winsorize_series(arr, low=WINSOR_LO, high=WINSOR_HI):\n",
        "    lo = np.nanpercentile(arr, low)\n",
        "    hi = np.nanpercentile(arr, high)\n",
        "    return np.clip(arr, lo, hi)\n",
        "\n",
        "# ---------- Step 1: read daily CSVs ----------\n",
        "daily_sets = read_daily_csvs(WORKDIR)\n",
        "print(\"Beolvasott napi sorok (orszÃ¡gok):\", list(daily_sets.keys()))\n",
        "\n",
        "assets = ensure_assets()\n",
        "exposure_df = load_exposure_matrix()\n",
        "vparams = load_vuln_params()\n",
        "\n",
        "processing_log = {\"start\": str(datetime.utcnow()), \"files\": list(daily_sets.keys()), \"reference_period\": f\"{REFERENCE_START}-{REFERENCE_END}\"}\n",
        "\n",
        "# ---------- Step 2: compute annual H_raw and P_raw per region (from daily regional CSV) ----------\n",
        "hazard_yearly = {}  # hazard -> DataFrame with columns: year, region, H_raw, P_raw\n",
        "\n",
        "for haz, meta in HAZARD_MAP.items():\n",
        "    rows = []\n",
        "    for region, df in daily_sets.items():\n",
        "        # ensure date index\n",
        "        ts = df.copy()\n",
        "        ts.index = pd.to_datetime(ts.index)\n",
        "        # compute per-year metrics\n",
        "        years = ts.index.year.unique()\n",
        "        for y in sorted(years):\n",
        "            sub = ts[ts.index.year == y]\n",
        "            if sub.shape[0] == 0:\n",
        "                continue\n",
        "            # find variable\n",
        "            if haz == \"windstorm\":\n",
        "                # pick best candidate available\n",
        "                candidates = meta.get(\"candidates\", [])\n",
        "                available = [c for c in candidates if c in sub.columns]\n",
        "                if len(available) == 0:\n",
        "                    H_raw = np.nan\n",
        "                    P_raw = np.nan\n",
        "                else:\n",
        "                    # intensity = p99 of the selected var (use max across candidates)\n",
        "                    arr = sub[available].max(axis=1).values\n",
        "                    H_raw = meta[\"intensity_func\"](arr)\n",
        "                    freq_mask = meta[\"freq_threshold_func\"](sub)\n",
        "                    P_raw = freq_mask.sum()  # days per year exceeding threshold\n",
        "            else:\n",
        "                varname = meta[\"var\"]\n",
        "                if varname not in sub.columns:\n",
        "                    H_raw = np.nan\n",
        "                    P_raw = np.nan\n",
        "                else:\n",
        "                    arr = sub[varname].values\n",
        "                    H_raw = meta[\"intensity_func\"](arr)\n",
        "                    freq_mask = meta[\"freq_threshold_func\"](sub)\n",
        "                    P_raw = freq_mask.sum()\n",
        "            rows.append({\"hazard\":haz,\"region\":region,\"year\":int(y),\"H_raw\":float(H_raw) if not np.isnan(H_raw) else np.nan,\"P_raw\":float(P_raw) if not np.isnan(P_raw) else np.nan})\n",
        "    hazard_yearly[haz] = pd.DataFrame(rows)\n",
        "    # save\n",
        "    hazard_yearly[haz].to_csv(os.path.join(OUTPUT_DIR, f\"hazard_yearly_{haz}.csv\"), index=False)\n",
        "    print(f\"Hazard {haz}: Ã©ves fÃ¡jl mentve: hazard_yearly_{haz}.csv, sorok: {len(hazard_yearly[haz])}\")\n",
        "\n",
        "# ---------- Step 3: Winsorize + reference-period min/max logging ----------\n",
        "# For each hazard, compute winsorized H and P over the reference period (or over available period if missing)\n",
        "norm_info = {}\n",
        "for haz, df in hazard_yearly.items():\n",
        "    if df.empty:\n",
        "        continue\n",
        "    # pivot years for reference slicing\n",
        "    ref_df = df[(df.year >= REFERENCE_START) & (df.year <= REFERENCE_END)]\n",
        "    if ref_df.empty:\n",
        "        ref_df = df  # fallback\n",
        "        processing_log.setdefault(\"notes\",[]).append(f\"{haz}: missing reference years {REFERENCE_START}-{REFERENCE_END}, fallback to available range\")\n",
        "    # winsorize H_raw and P_raw using reference rows per region separately\n",
        "    df2 = df.copy()\n",
        "    df2[\"H_win\"] = np.nan\n",
        "    df2[\"P_win\"] = np.nan\n",
        "    for region in df2.region.unique():\n",
        "        mask = df2.region==region\n",
        "        subref = ref_df[ref_df.region==region]\n",
        "        if subref.empty:\n",
        "            # no ref, use whole region series\n",
        "            subref = df2[mask]\n",
        "        # winsor bounds based on reference\n",
        "        H_lo = np.nanpercentile(subref[\"H_raw\"].dropna(), WINSOR_LO) if subref[\"H_raw\"].dropna().size>0 else np.nan\n",
        "        H_hi = np.nanpercentile(subref[\"H_raw\"].dropna(), WINSOR_HI) if subref[\"H_raw\"].dropna().size>0 else np.nan\n",
        "        P_lo = np.nanpercentile(subref[\"P_raw\"].dropna(), WINSOR_LO) if subref[\"P_raw\"].dropna().size>0 else np.nan\n",
        "        P_hi = np.nanpercentile(subref[\"P_raw\"].dropna(), WINSOR_HI) if subref[\"P_raw\"].dropna().size>0 else np.nan\n",
        "        # apply winsorize to that region's rows\n",
        "        df2.loc[mask,\"H_win\"] = np.clip(df2.loc[mask,\"H_raw\"], H_lo, H_hi) if (not np.isnan(H_lo) and not np.isnan(H_hi)) else df2.loc[mask,\"H_raw\"]\n",
        "        df2.loc[mask,\"P_win\"] = np.clip(df2.loc[mask,\"P_raw\"], P_lo, P_hi) if (not np.isnan(P_lo) and not np.isnan(P_hi)) else df2.loc[mask,\"P_raw\"]\n",
        "        norm_info.setdefault(haz,{}).setdefault(region,{\"H_lo\":H_lo,\"H_hi\":H_hi,\"P_lo\":P_lo,\"P_hi\":P_hi})\n",
        "    # min-max normalization across ref period for H_win and P_win\n",
        "    df2[\"H_norm\"] = np.nan\n",
        "    df2[\"P_norm\"] = np.nan\n",
        "    for region in df2.region.unique():\n",
        "        mask = df2.region==region\n",
        "        subref = ref_df[ref_df.region==region]\n",
        "        if subref.empty:\n",
        "            subref = df2[mask]\n",
        "        Hmin = np.nanmin(subref[\"H_win\"]) if subref[\"H_win\"].dropna().size>0 else np.nan\n",
        "        Hmax = np.nanmax(subref[\"H_win\"]) if subref[\"H_win\"].dropna().size>0 else np.nan\n",
        "        Pmin = np.nanmin(subref[\"P_win\"]) if subref[\"P_win\"].dropna().size>0 else np.nan\n",
        "        Pmax = np.nanmax(subref[\"P_win\"]) if subref[\"P_win\"].dropna().size>0 else np.nan\n",
        "        # store for log\n",
        "        norm_info[haz][region].update({\"Hmin\":Hmin,\"Hmax\":Hmax,\"Pmin\":Pmin,\"Pmax\":Pmax})\n",
        "        # apply norm with safe handling\n",
        "        if not np.isnan(Hmin) and not np.isnan(Hmax) and (Hmax>Hmin):\n",
        "            df2.loc[mask,\"H_norm\"] = (df2.loc[mask,\"H_win\"] - Hmin) / (Hmax - Hmin)\n",
        "        else:\n",
        "            df2.loc[mask,\"H_norm\"] = np.nan\n",
        "        if not np.isnan(Pmin) and not np.isnan(Pmax) and (Pmax>Pmin):\n",
        "            df2.loc[mask,\"P_norm\"] = (df2.loc[mask,\"P_win\"] - Pmin) / (Pmax - Pmin)\n",
        "        else:\n",
        "            df2.loc[mask,\"P_norm\"] = np.nan\n",
        "    hazard_yearly[haz] = df2\n",
        "    hazard_yearly[haz].to_csv(os.path.join(OUTPUT_DIR, f\"hazard_yearly_{haz}_processed.csv\"), index=False)\n",
        "\n",
        "# save norm info\n",
        "with open(os.path.join(OUTPUT_DIR,\"norm_info.json\"),\"w\") as fh:\n",
        "    json.dump(norm_info, fh, default=lambda x: None, indent=2)\n",
        "\n",
        "# ---------- Step 4: Prepare Exposure matrix (E) ----------\n",
        "if exposure_df is None:\n",
        "    # Create default exposure: all assets fully exposed (E=1) to all hazards in same region\n",
        "    # We need to map region->asset; with real assets.csv you'd compute precise GIS overlays.\n",
        "    exp_rows = []\n",
        "    for _, a in assets.iterrows():\n",
        "        for haz in HAZARD_MAP.keys():\n",
        "            exp_rows.append({\"asset_id\":a[\"asset_id\"], \"hazard_id\":haz, \"E\":1.0})\n",
        "    exposure_df = pd.DataFrame(exp_rows)\n",
        "    exposure_df.to_csv(os.path.join(OUTPUT_DIR,\"exposure_matrix.csv\"), index=False)\n",
        "else:\n",
        "    exposure_df.to_csv(os.path.join(OUTPUT_DIR,\"exposure_matrix.csv\"), index=False)\n",
        "print(\"Exposure matrix saved.\")\n",
        "\n",
        "# ---------- Step 5: Vulnerability matrix (V) ----------\n",
        "# Build V per asset-hazard using V0 baseline and asset metadata. We'll create entries for all asset-hazard combos.\n",
        "v0 = vparams[\"defaults\"]\n",
        "age_scale_factor = vparams.get(\"age_scale_factor\",0.5)\n",
        "redundancy_factor = vparams.get(\"redundancy_factor\",0.3)\n",
        "maintenance_factor = vparams.get(\"maintenance_factor\",0.4)\n",
        "\n",
        "v_rows = []\n",
        "for _, a in assets.iterrows():\n",
        "    asset_type = a.get(\"type\",\"OffshoreWind\")\n",
        "    V0 = v0.get(asset_type, 0.5)\n",
        "    # derive age_scale from commission_year if present\n",
        "    if not pd.isna(a.get(\"commission_year\")):\n",
        "        age = max(0, 2025 - int(a.get(\"commission_year\")))  # relative age\n",
        "        age_scale = min(1.0, age / 30.0)  # scale 0-1 over 30 years\n",
        "    else:\n",
        "        age_scale = 0.5\n",
        "    # placeholders for redundancy and maintenance if not present\n",
        "    redundancy = float(a.get(\"redundancy\", 0.0)) if not pd.isna(a.get(\"redundancy\",np.nan)) else 0.0\n",
        "    maintenance_score = float(a.get(\"maintenance_score\", 0.5)) if not pd.isna(a.get(\"maintenance_score\",np.nan)) else 0.5\n",
        "    for haz in HAZARD_MAP.keys():\n",
        "        V = V0 * (1 + age_scale_factor * age_scale) * (1 - redundancy_factor * redundancy) * (1 - maintenance_factor * maintenance_score)\n",
        "        V = max(0.0, min(1.0, V))\n",
        "        v_rows.append({\"asset_id\":a[\"asset_id\"], \"hazard_id\":haz, \"V\":V, \"V0\":V0, \"age_scale\":age_scale, \"redundancy\":redundancy, \"maintenance_score\":maintenance_score})\n",
        "vuln_df = pd.DataFrame(v_rows)\n",
        "vuln_df.to_csv(os.path.join(OUTPUT_DIR,\"vulnerability_matrix.csv\"), index=False)\n",
        "print(\"Vulnerability matrix saved.\")\n",
        "\n",
        "# ---------- Step 6: Combine and compute r_j,i,t with MICE imputations + MC ----------\n",
        "# Build a combined table of asset x hazard x year with H_norm, P_norm, E, V\n",
        "all_results = []\n",
        "for haz, df in hazard_yearly.items():\n",
        "    for _, row in df.iterrows():\n",
        "        year = int(row[\"year\"])\n",
        "        region = row[\"region\"]\n",
        "        Hn = row[\"H_norm\"]\n",
        "        Pn = row[\"P_norm\"]\n",
        "        # find assets in same region (best-effort: match by asset_id prefix)\n",
        "        # If assets don't have region, we assign all assets (or only those matching region prefix)\n",
        "        candidate_assets = assets[assets[\"asset_id\"].str.contains(region, na=False)]\n",
        "        if candidate_assets.empty:\n",
        "            candidate_assets = assets  # fallback: all\n",
        "        for _, a in candidate_assets.iterrows():\n",
        "            aid = a[\"asset_id\"]\n",
        "            # E\n",
        "            e_row = exposure_df[(exposure_df.asset_id==aid) & (exposure_df.hazard_id==haz)]\n",
        "            if not e_row.empty:\n",
        "                E = float(e_row.iloc[0][\"E\"])\n",
        "            else:\n",
        "                E = np.nan\n",
        "            # V\n",
        "            v_row = vuln_df[(vuln_df.asset_id==aid) & (vuln_df.hazard_id==haz)]\n",
        "            V = float(v_row.iloc[0][\"V\"]) if not v_row.empty else np.nan\n",
        "            all_results.append({\"asset_id\":aid,\"hazard_id\":haz,\"year\":year,\"region\":region,\"H_norm\":Hn,\"P_norm\":Pn,\"E\":E,\"V\":V})\n",
        "\n",
        "combined = pd.DataFrame(all_results)\n",
        "combined.to_csv(os.path.join(OUTPUT_DIR,\"combined_asset_hazard_raw.csv\"), index=False)\n",
        "print(\"Combined raw table saved.\")\n",
        "\n",
        "# Prepare matrix for imputations: select columns of interest\n",
        "imp_cols = [\"H_norm\",\"P_norm\",\"E\",\"V\"]\n",
        "imp_df = combined[imp_cols].copy()\n",
        "\n",
        "# Flag missing entries\n",
        "missing_mask = imp_df.isna()\n",
        "missing_summary = missing_mask.sum().to_dict()\n",
        "processing_log[\"missing_summary_before_impute\"] = missing_summary\n",
        "\n",
        "# MICE (IterativeImputer) multiple imputations\n",
        "rng = np.random.RandomState(MC_SEED)\n",
        "imp_results = []  # store per-imputation full r values\n",
        "imputer = IterativeImputer(estimator=BayesianRidge(), random_state=MC_SEED, max_iter=20, sample_posterior=True)\n",
        "\n",
        "for m in range(M_IMPUTE):\n",
        "    # fit/transform\n",
        "    try:\n",
        "        imputed = imputer.fit_transform(imp_df)  # shape (n_samples, 4)\n",
        "    except Exception as e:\n",
        "        print(\"Imputer error:\", e)\n",
        "        # fallback: simple median fill\n",
        "        imputed = imp_df.fillna(imp_df.median()).values\n",
        "    imputed_df = pd.DataFrame(imputed, columns=imp_cols)\n",
        "    # compute h = H_norm * P_norm\n",
        "    imputed_df[\"h\"] = imputed_df[\"H_norm\"] * imputed_df[\"P_norm\"]\n",
        "    # compute r = h * E * V per row\n",
        "    imputed_df[\"r\"] = imputed_df[\"h\"] * imputed_df[\"E\"] * imputed_df[\"V\"]\n",
        "    imputed_df[\"impute_run\"] = m\n",
        "    imp_results.append(imputed_df)\n",
        "\n",
        "# merge imputed runs into combined indexes\n",
        "all_imputations = []\n",
        "for m, impd in enumerate(imp_results):\n",
        "    dfm = combined[[\"asset_id\",\"hazard_id\",\"year\",\"region\"]].reset_index(drop=True).copy()\n",
        "    dfm = pd.concat([dfm, impd.reset_index(drop=True)], axis=1)\n",
        "    all_imputations.append(dfm)\n",
        "\n",
        "all_imp_df = pd.concat(all_imputations, ignore_index=True)\n",
        "all_imp_df.to_csv(os.path.join(OUTPUT_DIR,\"all_imputations_full.csv\"), index=False)\n",
        "\n",
        "# Monte-Carlo stats per asset/hazard/year\n",
        "mc_stats = all_imp_df.groupby([\"asset_id\",\"hazard_id\",\"year\"]).agg(\n",
        "    r_mean = (\"r\",\"mean\"),\n",
        "    r_std  = (\"r\",\"std\"),\n",
        "    r_p5   = (\"r\", lambda x: np.nanpercentile(x,5)),\n",
        "    r_p50  = (\"r\", \"median\"),\n",
        "    r_p95  = (\"r\", lambda x: np.nanpercentile(x,95)),\n",
        "    H_norm_mean=(\"H_norm\",\"mean\"),\n",
        "    P_norm_mean=(\"P_norm\",\"mean\"),\n",
        "    E_mean=(\"E\",\"mean\"),\n",
        "    V_mean=(\"V\",\"mean\")\n",
        ").reset_index()\n",
        "\n",
        "mc_stats.to_csv(os.path.join(OUTPUT_DIR,\"r_values_mc_stats.csv\"), index=False)\n",
        "\n",
        "# Save processing log & finish\n",
        "processing_log[\"end\"] = str(datetime.utcnow())\n",
        "processing_log[\"impute_runs\"] = M_IMPUTE\n",
        "processing_log[\"mc_seed\"] = MC_SEED\n",
        "with open(os.path.join(OUTPUT_DIR,\"processing_log.json\"),\"w\") as fh:\n",
        "    json.dump(processing_log, fh, indent=2)\n",
        "\n",
        "print(\"Done. Outputs in:\", OUTPUT_DIR)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 983
        },
        "id": "yPELrtObcLI0",
        "outputId": "82aea6da-5bd2-4238-b651-63f4020d67b4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (1.16.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.12/dist-packages (3.1.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.12/dist-packages (from openpyxl) (2.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Beolvasott napi sorok (orszÃ¡gok): ['dania', 'nemet', 'tajvan', 'uk', 'usa']\n",
            "Hazard heatwave: Ã©ves fÃ¡jl mentve: hazard_yearly_heatwave.csv, sorok: 120\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2807143435.py:125: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  processing_log = {\"start\": str(datetime.utcnow()), \"files\": list(daily_sets.keys()), \"reference_period\": f\"{REFERENCE_START}-{REFERENCE_END}\"}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hazard flood: Ã©ves fÃ¡jl mentve: hazard_yearly_flood.csv, sorok: 120\n",
            "Hazard windstorm: Ã©ves fÃ¡jl mentve: hazard_yearly_windstorm.csv, sorok: 120\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'H_win'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3805\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3806\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'H_win'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2807143435.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msubref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m             \u001b[0msubref\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m         \u001b[0mHmin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnanmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubref\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"H_win\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msubref\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"H_win\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m         \u001b[0mHmax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnanmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubref\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"H_win\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msubref\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"H_win\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0mPmin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnanmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubref\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"P_win\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msubref\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"P_win\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4102\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4104\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3810\u001b[0m             ):\n\u001b[1;32m   3811\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3814\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'H_win'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "\n",
        "# betÃ¶ltjÃ¼k az Ã¶sszes hazard outputot\n",
        "hazard_files = glob.glob(\"/content/cpri_outputs/hazard_yearly_*.csv\")\n",
        "\n",
        "for file in hazard_files:\n",
        "    df = pd.read_csv(file)\n",
        "    print(f\"ğŸ§© JavÃ­tjuk: {file}\")\n",
        "\n",
        "    # ha nincs H_win, de van H_raw â†’ mÃ¡soljuk Ã¡t\n",
        "    if 'H_win' not in df.columns and 'H_raw' in df.columns:\n",
        "        df['H_win'] = df['H_raw']\n",
        "    if 'P_win' not in df.columns and 'P_raw' in df.columns:\n",
        "        df['P_win'] = df['P_raw']\n",
        "\n",
        "    # ha van normalizÃ¡lÃ¡s, de nincs winsor: ezutÃ¡n ÃºjraszÃ¡moljuk\n",
        "    if 'H_norm' not in df.columns:\n",
        "        df['H_norm'] = (df['H_win'] - df['H_win'].min()) / (df['H_win'].max() - df['H_win'].min())\n",
        "    if 'P_norm' not in df.columns:\n",
        "        df['P_norm'] = (df['P_win'] - df['P_win'].min()) / (df['P_win'].max() - df['P_win'].min())\n",
        "\n",
        "    # mentsÃ¼k vissza\n",
        "    df.to_csv(file, index=False)\n",
        "    print(f\"âœ… Mentve javÃ­tva: {file}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jGoUS_PflB-w",
        "outputId": "b2b941da-6d05-46e9-f6a5-8c2371de0a0c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ§© JavÃ­tjuk: /content/cpri_outputs/hazard_yearly_flood.csv\n",
            "âœ… Mentve javÃ­tva: /content/cpri_outputs/hazard_yearly_flood.csv\n",
            "ğŸ§© JavÃ­tjuk: /content/cpri_outputs/hazard_yearly_heatwave.csv\n",
            "âœ… Mentve javÃ­tva: /content/cpri_outputs/hazard_yearly_heatwave.csv\n",
            "ğŸ§© JavÃ­tjuk: /content/cpri_outputs/hazard_yearly_windstorm.csv\n",
            "âœ… Mentve javÃ­tva: /content/cpri_outputs/hazard_yearly_windstorm.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!head -n 5 /content/cpri_outputs/hazard_yearly_heatwave.csv\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VzJarbSrls7z",
        "outputId": "0588de67-2c9b-47f3-d55a-366354aecf92"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hazard,region,year,H_raw,P_raw,H_win,P_win,H_norm,P_norm\n",
            "heatwave,dania,2000,,,,,,\n",
            "heatwave,dania,2001,,,,,,\n",
            "heatwave,dania,2002,,,,,,\n",
            "heatwave,dania,2003,,,,,,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# beolvassa a docx-tÃ¡blÃ¡t\n",
        "table = pd.read_html(\"/content/assets.docx\")[0]\n",
        "# menti CSV-be\n",
        "table.to_csv(\"/content/assets.csv\", index=False)\n",
        "print(\"âœ… assets.csv elkÃ©szÃ¼lt!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "WyJWNk5HBH6f",
        "outputId": "5119df79-74df-4700-da15-1304e9603ee2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "UnicodeDecodeError",
          "evalue": "'utf-8' codec can't decode byte 0xa5 in position 48: invalid start byte",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2862683310.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# beolvassa a docx-tÃ¡blÃ¡t\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_html\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/assets.docx\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m# menti CSV-be\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/assets.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/html.py\u001b[0m in \u001b[0;36mread_html\u001b[0;34m(io, match, flavor, header, index_col, skiprows, attrs, parse_dates, thousands, encoding, decimal, converters, na_values, keep_default_na, displayed_only, extract_links, dtype_backend, storage_options)\u001b[0m\n\u001b[1;32m   1238\u001b[0m         )\n\u001b[1;32m   1239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1240\u001b[0;31m     return _parse(\n\u001b[0m\u001b[1;32m   1241\u001b[0m         \u001b[0mflavor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflavor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1242\u001b[0m         \u001b[0mio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/html.py\u001b[0m in \u001b[0;36m_parse\u001b[0;34m(flavor, io, match, attrs, encoding, displayed_only, extract_links, storage_options, **kwargs)\u001b[0m\n\u001b[1;32m   1001\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1002\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mretained\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# for mypy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1003\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mretained\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1004\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m     \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/html.py\u001b[0m in \u001b[0;36m_parse\u001b[0;34m(flavor, io, match, attrs, encoding, displayed_only, extract_links, storage_options, **kwargs)\u001b[0m\n\u001b[1;32m    981\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    982\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 983\u001b[0;31m             \u001b[0mtables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_tables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    984\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcaught\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m             \u001b[0;31m# if `io` is an io-like object, check if it's seekable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/html.py\u001b[0m in \u001b[0;36mparse_tables\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mparsed\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfooter\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mtuples\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtables\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m         \"\"\"\n\u001b[0;32m--> 249\u001b[0;31m         \u001b[0mtables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_tables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_thead_tbody_tfoot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtable\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/html.py\u001b[0m in \u001b[0;36m_build_doc\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    651\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mbs4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 653\u001b[0;31m         \u001b[0mbdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setup_build_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    654\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m             \u001b[0mudoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/html.py\u001b[0m in \u001b[0;36m_setup_build_doc\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    643\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setup_build_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 645\u001b[0;31m         \u001b[0mraw_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    646\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mraw_text\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"No text parsed from document: {self.io}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/html.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(obj, encoding, storage_options)\u001b[0m\n\u001b[1;32m    144\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         ) as handles:\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n",
            "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xa5 in position 48: invalid start byte"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-docx\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zm6C4_gtCaF5",
        "outputId": "0ed2d7ee-e003-43ca-b1a1-0106d1a2b4fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-docx\n",
            "  Downloading python_docx-1.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from python-docx) (5.4.0)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.12/dist-packages (from python-docx) (4.15.0)\n",
            "Downloading python_docx-1.2.0-py3-none-any.whl (252 kB)\n",
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/253.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m\u001b[90mâ”\u001b[0m \u001b[32m245.8/253.0 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-docx\n",
            "Successfully installed python-docx-1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/era5_processed_light && mv /content/*_daily_light.csv /content/era5_processed_light/\n"
      ],
      "metadata": {
        "id": "VwHKuU6s7xZw"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from docx import Document\n",
        "import pandas as pd\n",
        "\n",
        "# Beolvassuk a Word-dokumentumot\n",
        "doc = Document(\"/content/assets.docx\")\n",
        "\n",
        "# KivesszÃ¼k az elsÅ‘ tÃ¡blÃ¡t\n",
        "table = doc.tables[0]\n",
        "\n",
        "# Sorok beolvasÃ¡sa\n",
        "data = []\n",
        "for row in table.rows:\n",
        "    data.append([cell.text.strip() for cell in row.cells])\n",
        "\n",
        "# DataFrame lÃ©trehozÃ¡sa Ã©s mentÃ©se CSV-be\n",
        "df = pd.DataFrame(data[1:], columns=data[0])  # elsÅ‘ sor = fejlÃ©c\n",
        "df.to_csv(\"/content/assets.csv\", index=False)\n",
        "print(\"âœ… assets.csv elkÃ©szÃ¼lt:\", df.shape[0], \"sor, mentve a /content-be\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "svyr8Z11ChFg",
        "outputId": "66a046f1-0d66-4c63-daf4-fdb533e10c76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "list index out of range",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4262743640.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# KivesszÃ¼k az elsÅ‘ tÃ¡blÃ¡t\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtables\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Sorok beolvasÃ¡sa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from docx import Document\n",
        "import pandas as pd\n",
        "\n",
        "# Beolvassuk a fÃ¡jlt\n",
        "doc = Document(\"/content/assets.docx\")\n",
        "\n",
        "rows = []\n",
        "for p in doc.paragraphs:\n",
        "    line = p.text.strip()\n",
        "    if \"|\" in line and not line.startswith(\"|-\"):\n",
        "        parts = [x.strip() for x in line.split(\"|\") if x.strip()]\n",
        "        # csak azokat a sorokat vesszÃ¼k, ahol legalÃ¡bb 5 oszlop van (igazi adat)\n",
        "        if len(parts) > 5:\n",
        "            rows.append(parts)\n",
        "\n",
        "# fejlÃ©c Ã©s adatok szÃ©tvÃ¡lasztÃ¡sa\n",
        "header = rows[0]\n",
        "data = [r for r in rows[1:] if len(r) == len(header)]  # kiszÅ±rjÃ¼k a hibÃ¡s sorokat\n",
        "\n",
        "# DataFrame\n",
        "df = pd.DataFrame(data, columns=header)\n",
        "\n",
        "# MentÃ©s\n",
        "df.to_csv(\"/content/assets.csv\", index=False)\n",
        "print(f\"âœ… assets.csv elkÃ©szÃ¼lt! {df.shape[0]} sor, {df.shape[1]} oszlop mentve.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMX1hv6TF6V-",
        "outputId": "05159e84-c087-4ebb-84cb-629d3215448e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… assets.csv elkÃ©szÃ¼lt! 0 sor, 119 oszlop mentve.\n"
          ]
        }
      ]
    }
  ]
}
