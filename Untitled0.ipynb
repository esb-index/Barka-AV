{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOPELjT7koi+pWPKKsjoWLU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/esb-index/Barka-AV/blob/main/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================\n",
        "# ERA5 ZIP -> MERGE -> PROCESS pipeline (Colab)\n",
        "# ==========================\n",
        "# Másold be egy Colab cellába és futtasd.\n",
        "# Feltételezzük: a zip fájlok egy 'cds' alkönyvtárban vannak (pl. /content/cds)\n",
        "# Fájlok nevei: dania1.zip ... dania4.zip, nemet1..4.zip, uk1..4.zip, tajvan1..4.zip, usa1..4.zip\n",
        "!mkdir cds\n",
        "!unzip cds_all.zip -d cds\n",
        "!ls cds\n",
        "\n",
        "# 0) TELEPÍTÉSEK\n",
        "!pip install xarray netCDF4 cfgrib cftime dask[complete] fsspec aiohttp pandas numpy scipy tqdm\n",
        "\n",
        "# 1) KÖNYVTÁRSTRUKTÚRA ÉS ZIP KIBONTÁSA\n",
        "import os, glob, zipfile, shutil\n",
        "from tqdm import tqdm\n",
        "\n",
        "ROOT = \"/content\"               # Colab root - módosítsd ha Drive-ot használsz (pl. /content/drive/MyDrive/era5)\n",
        "ZIPDIR = os.path.join(ROOT, \"cds\")\n",
        "EXTRACT_DIR = os.path.join(ROOT, \"era5_extracted\")\n",
        "os.makedirs(EXTRACT_DIR, exist_ok=True)\n",
        "\n",
        "zip_files = sorted(glob.glob(os.path.join(ZIPDIR, \"*.zip\")))\n",
        "print(f\"Talált zip fájlok: {len(zip_files)}\")\n",
        "\n",
        "for zf in tqdm(zip_files, desc=\"Kibontás\"):\n",
        "    with zipfile.ZipFile(zf, 'r') as z:\n",
        "        # Kibontás egy alfolderbe az eredeti zip név alapján\n",
        "        base = os.path.splitext(os.path.basename(zf))[0]\n",
        "        outdir = os.path.join(EXTRACT_DIR, base)\n",
        "        os.makedirs(outdir, exist_ok=True)\n",
        "        z.extractall(outdir)\n",
        "\n",
        "print(\"Kibontás kész. Kimenet:\", EXTRACT_DIR)\n",
        "\n",
        "# 2) FÁJLFORMÁTUM ELLENŐRZÉSE (GRIB vs NETCDF)\n",
        "import pathlib\n",
        "all_files = sorted(glob.glob(os.path.join(EXTRACT_DIR, \"**\", \"*\"), recursive=True))\n",
        "nc_files = [f for f in all_files if f.endswith(\".nc\")]\n",
        "grib_files = [f for f in all_files if f.endswith(\".grib\") or f.endswith(\".grb\")]\n",
        "\n",
        "print(f\"NetCDF fájlok: {len(nc_files)}, GRIB fájlok: {len(grib_files)}\")\n",
        "\n",
        "# 3) Ha vannak GRIB fájlok: konvertáld NetCDF-re (xarray+cfgrib vagy cdo ha telepítve)\n",
        "# Megpróbáljuk cfgrib + xarray útvonalat: egyszerű és Pythonos. (Ha cfgrib gond van, érdemes CDO-t telepíteni.)\n",
        "import xarray as xr\n",
        "\n",
        "CONVERTED_DIR = os.path.join(ROOT, \"era5_nc\")\n",
        "os.makedirs(CONVERTED_DIR, exist_ok=True)\n",
        "\n",
        "def grib_to_nc(inpath, outdir=CONVERTED_DIR):\n",
        "    # out filename based on input\n",
        "    base = pathlib.Path(inpath).stem\n",
        "    outpath = os.path.join(outdir, base + \".nc\")\n",
        "    if os.path.exists(outpath):\n",
        "        return outpath\n",
        "    try:\n",
        "        # open with cfgrib engine and save to netcdf\n",
        "        ds = xr.open_dataset(inpath, engine=\"cfgrib\")\n",
        "        # Some GRIBs have multiple messages per file -> try to merge dims safely\n",
        "        ds.to_netcdf(outpath)\n",
        "        ds.close()\n",
        "        return outpath\n",
        "    except Exception as e:\n",
        "        print(f\"cfgrib conversion failed for {inpath}: {e}\")\n",
        "        return None\n",
        "\n",
        "if len(grib_files) > 0:\n",
        "    print(\"Konvertálás GRIB->NetCDF (cfgrib)...\")\n",
        "    for g in tqdm(grib_files):\n",
        "        grib_to_nc(g)\n",
        "\n",
        "# Frissítsük a NetCDF listát\n",
        "nc_files = sorted(glob.glob(os.path.join(EXTRACT_DIR, \"**\", \"*.nc\"), recursive=True)) \\\n",
        "           + sorted(glob.glob(os.path.join(CONVERTED_DIR, \"*.nc\")))\n",
        "nc_files = sorted(set(nc_files))\n",
        "print(f\"Összes NetCDF fájl (kibontott + konvertált): {len(nc_files)}\")\n",
        "\n",
        "# 4) REGION LABELLEZÉS (a zip mappanevek alapján)\n",
        "# Miután kibontottuk, az eredeti zip-könyvtár neve (pl 'dania1') megtalálható az útvonalban.\n",
        "def region_from_path(p):\n",
        "    p_low = p.lower()\n",
        "    if \"dania\" in p_low: return \"dania\"\n",
        "    if \"nemet\" in p_low: return \"nemet\"\n",
        "    if \"uk\" in p_low: return \"uk\"   # tartalmaz hollandot is per a te megjegyzésed\n",
        "    if \"tajvan\" in p_low: return \"tajvan\"\n",
        "    if \"usa\" in p_low: return \"usa\"\n",
        "    return \"other\"\n",
        "\n",
        "region_files = {}\n",
        "for f in nc_files:\n",
        "    r = region_from_path(f)\n",
        "    region_files.setdefault(r, []).append(f)\n",
        "\n",
        "for r, fl in region_files.items():\n",
        "    print(r, len(fl))\n",
        "\n",
        "# 5) VÁLTOZÓLISTA (amelyeket feldolgozunk) — unify: a legfontosabbak (európa/usa/tajvan)\n",
        "variables_needed = [\n",
        "    \"t2m\",   # 2m_temperature\n",
        "    \"tp\",    # total_precipitation\n",
        "    \"u10\", \"v10\",  # 10m u/v\n",
        "    \"msl\",   # mean_sea_level_pressure\n",
        "    \"ssrd\",  # surface_solar_radiation_downwards\n",
        "    \"sd\",    # snow_depth (nem kötelező minden régiónak, de ha van)\n",
        "    \"sst\",   # sea_surface_temperature\n",
        "    \"10fg\"   # gust (cfgrib variable name may vary; handle aliases)\n",
        "]\n",
        "# Megjegyzés: a NetCDF változónevek eltérhetnek (pl. '2m_temperature' vagy 't2m').\n",
        "# A feldolgozásnál megpróbálunk különböző aliasokat.\n",
        "\n",
        "# 6) HELYI SEGÉDFÜGGVÉNYEK: változó-ALIAS kezelés, winsorize, imputálás, deriváltak\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import stats\n",
        "\n",
        "def alias_map(varname):\n",
        "    # egyszerű alias leképezés a gyakori ERA5 nevekhez\n",
        "    m = {\n",
        "        \"t2m\": [\"t2m\", \"2m_temperature\", \"air_temperature_at_2m\"],\n",
        "        \"tp\": [\"tp\", \"total_precipitation\"],\n",
        "        \"u10\": [\"u10\", \"10m_u_component_of_wind\"],\n",
        "        \"v10\": [\"v10\", \"10m_v_component_of_wind\"],\n",
        "        \"msl\": [\"msl\", \"mean_sea_level_pressure\"],\n",
        "        \"ssrd\": [\"ssrd\", \"surface_solar_radiation_downwards\"],\n",
        "        \"sd\": [\"sd\", \"snow_depth\"],\n",
        "        \"sst\": [\"sst\", \"sea_surface_temperature\"],\n",
        "        \"10fg\": [\"10fg\", \"maximum_wind_gust_since_previous_post_processing\", \"10m_wind_gust\", \"gust\"]\n",
        "    }\n",
        "    return m.get(varname, [varname])\n",
        "\n",
        "def find_variable_in_ds(ds, logical_name):\n",
        "    for alias in alias_map(logical_name):\n",
        "        if alias in ds.variables:\n",
        "            return alias\n",
        "    return None\n",
        "\n",
        "def winsorize_series(arr, lower_q=0.01, upper_q=0.99):\n",
        "    # arr is 1D numpy; clip to percentiles\n",
        "    lo = np.nanpercentile(arr, lower_q*100)\n",
        "    hi = np.nanpercentile(arr, upper_q*100)\n",
        "    return np.clip(arr, lo, hi)\n",
        "\n",
        "# 7) REGIONÁLIS MERGE ÉS PROCESS (példányosított)\n",
        "OUT_DIR = os.path.join(ROOT, \"era5_processed\")\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "# Dask performance\n",
        "import dask\n",
        "dask.config.set({\"array.slicing.split_large_chunks\": False})\n",
        "# dask.config.set(scheduler='threads')\n",
        "\n",
        "for region, files in region_files.items():\n",
        "    if region == \"other\" or len(files) == 0:\n",
        "        continue\n",
        "    print(f\"\\n--- Feldolgozás: {region} ({len(files)} fájl) ---\")\n",
        "    # Megpróbáljuk egyszerre megnyitni őket\n",
        "    try:\n",
        "        ds = xr.open_mfdataset(files, combine='by_coords', parallel=True, concat_dim=\"time\")\n",
        "    except Exception as e:\n",
        "        print(f\"open_mfdataset hiba: {e}\\nPróbáljuk változónként összevonni.\")\n",
        "        # Alternatíva: változónként nyitjuk és merge-öljük\n",
        "        var_datasets = []\n",
        "        for v in variables_needed:\n",
        "            # keresünk az első fájlban alias-t\n",
        "            found = None\n",
        "            for f in files:\n",
        "                try:\n",
        "                    with xr.open_dataset(f) as tmp:\n",
        "                        alias = find_variable_in_ds(tmp, v)\n",
        "                        if alias:\n",
        "                            found = alias\n",
        "                            break\n",
        "                except Exception:\n",
        "                    continue\n",
        "            if not found:\n",
        "                continue\n",
        "            # most open_mfdataset csak ezzel a varra (usecols equivalent: use sel? use preprocess)\n",
        "            def preprocess(ds_in, varalias=found):\n",
        "                # keep only variable and time/coords to reduce mem\n",
        "                to_keep = [varalias] + [d for d in ds_in.dims.keys()] + [c for c in ds_in.coords.keys()]\n",
        "                # ensure time present\n",
        "                return ds_in[[varalias]]\n",
        "            try:\n",
        "                dd = xr.open_mfdataset(files, preprocess=preprocess, combine='by_coords', parallel=True, concat_dim=\"time\")\n",
        "                var_datasets.append(dd)\n",
        "            except Exception as e2:\n",
        "                print(f\"Változó {v} nyitása sikertelen: {e2}\")\n",
        "        # egyszerű merge\n",
        "        if len(var_datasets) == 0:\n",
        "            print(\"Nincs egyetlen megnyitható változó sem, továbblépünk.\")\n",
        "            continue\n",
        "        ds = xr.merge(var_datasets)\n",
        "\n",
        "    print(\"Dataset megnyitva. Változók:\", list(ds.variables.keys())[:30])\n",
        "\n",
        "    # 8) Alias normalizálás: átnevezzük a változókat logikus nevekre (t2m, u10 stb.) ha található\n",
        "    rename_map = {}\n",
        "    for logical in [\"t2m\",\"tp\",\"u10\",\"v10\",\"msl\",\"ssrd\",\"sd\",\"sst\",\"10fg\"]:\n",
        "        found = find_variable_in_ds(ds, logical)\n",
        "        if found:\n",
        "            rename_map[found] = logical\n",
        "    if rename_map:\n",
        "        ds = ds.rename(rename_map)\n",
        "        print(\"Átnevezett változók:\", rename_map)\n",
        "\n",
        "    # 9) IDŐTENGELY és CHUNK\n",
        "    # Győződj meg róla, hogy van time dim\n",
        "    if \"time\" not in ds.dims:\n",
        "        print(\"Nincs time dim, átugorva.\")\n",
        "        continue\n",
        "\n",
        "    # 10) Deriváltak: wind speed, gust daily max, wind@100m (power law), dMSLP/dt\n",
        "    # Wind speed (instant)\n",
        "    if (\"u10\" in ds.variables) and (\"v10\" in ds.variables):\n",
        "        ds[\"wind10\"] = np.sqrt(ds[\"u10\"]**2 + ds[\"v10\"]**2)\n",
        "\n",
        "    # Hub height extrapolation (simple power law) -> wind at 100m\n",
        "    # U_model at z: U_z = U_ref * (z/z_ref)**alpha. alpha=0.14 (neutral) default\n",
        "    z_ref = 10.0\n",
        "    z_target = 100.0\n",
        "    alpha = 0.14\n",
        "    if \"wind10\" in ds.variables:\n",
        "        ds[\"wind100\"] = ds[\"wind10\"] * (z_target / z_ref)**alpha\n",
        "\n",
        "    # dMSLP/dt (24h difference) -> requires hourly data; we compute a 24h centered diff if possible\n",
        "    if \"msl\" in ds.variables:\n",
        "        try:\n",
        "            ds = ds.assign_coords(time=ds.time)\n",
        "            dmsl = ds[\"msl\"].diff(\"time\", n=24)  # difference over 24 steps: only valid if hourly and continuous\n",
        "            # align by padding with NaNs to original length\n",
        "            ds[\"dmsl_24h\"] = dmsl.reindex_like(ds[\"msl\"])\n",
        "        except Exception:\n",
        "            # fallback: simple diff next-prev\n",
        "            ds[\"dmsl_1step\"] = ds[\"msl\"].diff(\"time\")\n",
        "\n",
        "    # daily aggregates: we resample to daily and compute daily max gust, daily precipitation sum, daily mean temp etc.\n",
        "    daily = ds.resample(time=\"1D\").agg({\n",
        "        v: (\"max\" if v in [\"10fg\",\"wind10\",\"wind100\"] else \"sum\" if v in [\"tp\"] else \"mean\")\n",
        "        for v in ds.data_vars\n",
        "    })\n",
        "    # Note: resample with dict requires xarray>=0.20; else we do simpler approach:\n",
        "    # For safety, fallback to explicit calcs if above fails\n",
        "    try:\n",
        "        daily = ds.resample(time=\"1D\").reduce(lambda x: x.max() if x.name in [\"10fg\",\"wind10\",\"wind100\"] else x.sum() if x.name==\"tp\" else x.mean())\n",
        "    except Exception:\n",
        "        # Simple commonly needed ones:\n",
        "        daily = xr.Dataset()\n",
        "        if \"10fg\" in ds.variables:\n",
        "            daily[\"daily_max_gust\"] = ds[\"10fg\"].resample(time=\"1D\").max()\n",
        "        if \"wind10\" in ds.variables:\n",
        "            daily[\"daily_max_wind10\"] = ds[\"wind10\"].resample(time=\"1D\").max()\n",
        "        if \"tp\" in ds.variables:\n",
        "            daily[\"daily_precip_sum\"] = ds[\"tp\"].resample(time=\"1D\").sum()\n",
        "        if \"t2m\" in ds.variables:\n",
        "            daily[\"daily_mean_t2m\"] = ds[\"t2m\"].resample(time=\"1D\").mean()\n",
        "\n",
        "    # 11) WINSORIZE (1-99%) és egyszerű imputáció (timewise interpolation then global median)\n",
        "    # We'll perform winsorize per variable over the whole region dataset (flatten)\n",
        "    def winsorize_dataset(ds_in, low_q=0.01, up_q=0.99):\n",
        "        for v in list(ds_in.data_vars):\n",
        "            try:\n",
        "                arr = ds_in[v].values\n",
        "                if np.isnan(arr).all():\n",
        "                    continue\n",
        "                lo = np.nanpercentile(arr, low_q*100)\n",
        "                hi = np.nanpercentile(arr, up_q*100)\n",
        "                arr_clipped = np.clip(arr, lo, hi)\n",
        "                ds_in[v].values = arr_clipped\n",
        "            except Exception as e:\n",
        "                print(f\"winsorize hiba {v}: {e}\")\n",
        "        return ds_in\n",
        "\n",
        "    daily = winsorize_dataset(daily, 0.01, 0.99)\n",
        "    # Impute: linear time interpolation per gridpoint if available, else fillna with median\n",
        "    def impute_dataset(ds_in):\n",
        "        for v in list(ds_in.data_vars):\n",
        "            try:\n",
        "                da = ds_in[v]\n",
        "                # convert to pandas in time dimension per gridpoint if small; else do xarray interpolation\n",
        "                da_interp = da.interpolate_na(dim=\"time\", method=\"linear\", fill_value=\"extrapolate\")\n",
        "                # still nan? fill with median\n",
        "                if np.isnan(da_interp.values).any():\n",
        "                    med = np.nanmedian(da_interp.values)\n",
        "                    da_interp = da_interp.fillna(med)\n",
        "                ds_in[v] = da_interp\n",
        "            except Exception as e:\n",
        "                print(f\"Imputáció hiba {v}: {e}\")\n",
        "        return ds_in\n",
        "\n",
        "    daily = impute_dataset(daily)\n",
        "\n",
        "    # 12) EXTRACT KEY INDICATORS TO CSV (per time and aggregated spatially: mean, max, percentiles)\n",
        "    # We'll compute region-aggregated series: region_mean, region_max, region_p95 for each daily var\n",
        "    import math\n",
        "    indicators = {}\n",
        "    for v in daily.data_vars:\n",
        "        da = daily[v]\n",
        "        # spatial aggregation: mean over lat/lon dims if present\n",
        "        spatial_dims = [d for d in da.dims if d not in [\"time\"]]\n",
        "        if len(spatial_dims) > 0:\n",
        "            regional_mean = da.mean(dim=spatial_dims)\n",
        "            regional_max = da.max(dim=spatial_dims)\n",
        "            regional_p95 = da.reduce(lambda x: np.nanpercentile(x, 95), dim=spatial_dims)\n",
        "        else:\n",
        "            regional_mean = da\n",
        "            regional_max = da\n",
        "            regional_p95 = da\n",
        "        # to pandas series\n",
        "        try:\n",
        "            df = pd.DataFrame({\n",
        "                f\"{v}_mean\": regional_mean.to_series(),\n",
        "                f\"{v}_max\": regional_max.to_series(),\n",
        "                f\"{v}_p95\": regional_p95.to_series()\n",
        "            })\n",
        "        except Exception:\n",
        "            # fallback: convert with .values\n",
        "            df = pd.DataFrame({\n",
        "                f\"{v}_mean\": regional_mean.values,\n",
        "                f\"{v}_max\": regional_max.values\n",
        "            }, index=pd.to_datetime(daily.time.values))\n",
        "        # append or merge\n",
        "        if len(indicators)==0:\n",
        "            indicators = df\n",
        "        else:\n",
        "            indicators = indicators.join(df, how=\"outer\")\n",
        "\n",
        "    # 13) Mentés: napi indicátor CSV, teljes regionális NetCDF (ha ok)\n",
        "    csv_out = os.path.join(OUT_DIR, f\"{region}_daily_indicators.csv\")\n",
        "    indicators.to_csv(csv_out)\n",
        "    print(f\"Regionális indikátor CSV kimentve: {csv_out}\")\n",
        "\n",
        "    # netcdf kimenet (daily)\n",
        "    netcdf_out = os.path.join(OUT_DIR, f\"{region}_daily.nc\")\n",
        "    try:\n",
        "        daily.to_netcdf(netcdf_out)\n",
        "        print(f\"Regionális daily NetCDF kimenet: {netcdf_out}\")\n",
        "    except Exception as e:\n",
        "        print(f\"NetCDF mentés hiba: {e}\")\n",
        "\n",
        "    # cleanup ds to free memory\n",
        "    try:\n",
        "        ds.close()\n",
        "        del ds\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "print(\"\\n==== Pipeline kész. Ellenőrizd az 'era5_processed' mappát. ====\")\n"
      ],
      "metadata": {
        "id": "-q3-WA4xGiM-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "196652af-fa90-47a2-e34e-a5459d90a1bf"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  cds_all.zip\n",
            "  End-of-central-directory signature not found.  Either this file is not\n",
            "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n",
            "  latter case the central directory and zipfile comment will be found on\n",
            "  the last disk(s) of this archive.\n",
            "unzip:  cannot find zipfile directory in one of cds_all.zip or\n",
            "        cds_all.zip.zip, and cannot find cds_all.zip.ZIP, period.\n",
            "Requirement already satisfied: xarray in /usr/local/lib/python3.12/dist-packages (2025.10.1)\n",
            "Collecting netCDF4\n",
            "  Downloading netcdf4-1.7.3-cp311-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.9 kB)\n",
            "Collecting cfgrib\n",
            "  Downloading cfgrib-0.9.15.1-py3-none-any.whl.metadata (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.1/56.1 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cftime\n",
            "  Downloading cftime-1.6.5-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (2025.3.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (3.13.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (1.16.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: dask[complete] in /usr/local/lib/python3.12/dist-packages (2025.5.0)\n",
            "Requirement already satisfied: packaging>=24.1 in /usr/local/lib/python3.12/dist-packages (from xarray) (25.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from netCDF4) (2025.10.5)\n",
            "Requirement already satisfied: attrs>=19.2 in /usr/local/lib/python3.12/dist-packages (from cfgrib) (25.4.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from cfgrib) (8.3.0)\n",
            "Collecting eccodes>=0.9.8 (from cfgrib)\n",
            "  Downloading eccodes-2.44.0-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: cloudpickle>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from dask[complete]) (3.1.1)\n",
            "Requirement already satisfied: partd>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from dask[complete]) (1.4.2)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from dask[complete]) (6.0.3)\n",
            "Requirement already satisfied: toolz>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from dask[complete]) (0.12.1)\n",
            "Requirement already satisfied: pyarrow>=14.0.1 in /usr/local/lib/python3.12/dist-packages (from dask[complete]) (18.1.0)\n",
            "Collecting lz4>=4.3.2 (from dask[complete])\n",
            "  Downloading lz4-4.4.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp) (1.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp) (1.22.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.12/dist-packages (from aiosignal>=1.4.0->aiohttp) (4.15.0)\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.12/dist-packages (from eccodes>=0.9.8->cfgrib) (2.0.0)\n",
            "Collecting findlibs (from eccodes>=0.9.8->cfgrib)\n",
            "  Downloading findlibs-0.1.2-py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting eccodeslib (from eccodes>=0.9.8->cfgrib)\n",
            "  Downloading eccodeslib-2.44.0.4-1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: locket in /usr/local/lib/python3.12/dist-packages (from partd>=1.4.0->dask[complete]) (1.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.12/dist-packages (from yarl<2.0,>=1.17.0->aiohttp) (3.10)\n",
            "Requirement already satisfied: distributed==2025.5.0 in /usr/local/lib/python3.12/dist-packages (from dask[complete]) (2025.5.0)\n",
            "Requirement already satisfied: bokeh>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from dask[complete]) (3.7.3)\n",
            "Requirement already satisfied: jinja2>=2.10.3 in /usr/local/lib/python3.12/dist-packages (from dask[complete]) (3.1.6)\n",
            "Requirement already satisfied: msgpack>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from distributed==2025.5.0->dask[complete]) (1.1.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from distributed==2025.5.0->dask[complete]) (5.9.5)\n",
            "Requirement already satisfied: sortedcontainers>=2.0.5 in /usr/local/lib/python3.12/dist-packages (from distributed==2025.5.0->dask[complete]) (2.4.0)\n",
            "Requirement already satisfied: tblib>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from distributed==2025.5.0->dask[complete]) (3.1.0)\n",
            "Requirement already satisfied: tornado>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from distributed==2025.5.0->dask[complete]) (6.4.2)\n",
            "Requirement already satisfied: urllib3>=1.26.5 in /usr/local/lib/python3.12/dist-packages (from distributed==2025.5.0->dask[complete]) (2.5.0)\n",
            "Requirement already satisfied: zict>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from distributed==2025.5.0->dask[complete]) (3.0.0)\n",
            "Requirement already satisfied: contourpy>=1.2 in /usr/local/lib/python3.12/dist-packages (from bokeh>=3.1.0->dask[complete]) (1.3.3)\n",
            "Requirement already satisfied: narwhals>=1.13 in /usr/local/lib/python3.12/dist-packages (from bokeh>=3.1.0->dask[complete]) (2.7.0)\n",
            "Requirement already satisfied: pillow>=7.1.0 in /usr/local/lib/python3.12/dist-packages (from bokeh>=3.1.0->dask[complete]) (11.3.0)\n",
            "Requirement already satisfied: xyzservices>=2021.09.1 in /usr/local/lib/python3.12/dist-packages (from bokeh>=3.1.0->dask[complete]) (2025.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2>=2.10.3->dask[complete]) (3.0.3)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi->eccodes>=0.9.8->cfgrib) (2.23)\n",
            "Collecting eckitlib==1.32.2.4 (from eccodeslib->eccodes>=0.9.8->cfgrib)\n",
            "  Downloading eckitlib-1.32.2.4-1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
            "Collecting fckitlib==0.14.0.4 (from eccodeslib->eccodes>=0.9.8->cfgrib)\n",
            "  Downloading fckitlib-0.14.0.4-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.1 kB)\n",
            "Downloading netcdf4-1.7.3-cp311-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (9.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m85.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cfgrib-0.9.15.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cftime-1.6.5-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m50.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading eccodes-2.44.0-py3-none-any.whl (83 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.5/83.5 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lz4-4.4.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading eccodeslib-2.44.0.4-1-cp312-cp312-manylinux_2_28_x86_64.whl (20.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.9/20.9 MB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading eckitlib-1.32.2.4-1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (44.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 MB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fckitlib-0.14.0.4-cp312-cp312-manylinux_2_28_x86_64.whl (13.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.4/13.4 MB\u001b[0m \u001b[31m112.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading findlibs-0.1.2-py3-none-any.whl (10 kB)\n",
            "Installing collected packages: findlibs, eckitlib, lz4, fckitlib, cftime, netCDF4, eccodeslib, eccodes, cfgrib\n",
            "Successfully installed cfgrib-0.9.15.1 cftime-1.6.5 eccodes-2.44.0 eccodeslib-2.44.0.4 eckitlib-1.32.2.4 fckitlib-0.14.0.4 findlibs-0.1.2 lz4-4.4.4 netCDF4-1.7.3\n",
            "Talált zip fájlok: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Kibontás: 0it [00:00, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kibontás kész. Kimenet: /content/era5_extracted\n",
            "NetCDF fájlok: 0, GRIB fájlok: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Összes NetCDF fájl (kibontott + konvertált): 0\n",
            "\n",
            "==== Pipeline kész. Ellenőrizd az 'era5_processed' mappát. ====\n"
          ]
        }
      ]
    }
  ]
}