{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOC3aCADiyqriJQUGVkQV1K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/esb-index/Barka-AV/blob/main/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/era5_processed_light && mv /content/*_daily_light.csv /content/era5_processed_light/\n"
      ],
      "metadata": {
        "id": "VwHKuU6s7xZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "3e_o6J8ypxw-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 6-hazard generator + E, V, r szÃ¡mÃ­tÃ¡s (Colab cella)\n",
        "import os, glob\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# ---------- BEÃLLÃTÃSOK ----------\n",
        "INPUT_DIR = \"/content\"\n",
        "OUTPUT_DIR = \"/content/cpri_outputs\"\n",
        "ASSETS_PATH = \"/content/cpri_outputs/assets.xlsx\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# region sÃºlyok (ahogy megadtad)\n",
        "region_weights = {\n",
        "    \"dania\": 0.8, \"denmark\": 0.8,\n",
        "    \"nemet\": 0.6, \"germany\": 0.6,\n",
        "    \"uk\": 0.9, \"netherlands\": 0.9,\n",
        "    \"usa\": 1.0, \"united states\": 1.0,\n",
        "    \"tajvan\": 1.0, \"taiwan\": 1.0\n",
        "}\n",
        "\n",
        "# fizikai (kÃ¡rokozÃ³) kÃ¼szÃ¶bÃ¶k (ha van ilyen fizikailag Ã©rtelmezett)\n",
        "ABS_THRESH = {\n",
        "    \"flood\": {\"tp_mm_day\": 50.0},               # napi csapadÃ©k > 50 mm -> potenciÃ¡lisan kÃ¡rokozÃ³\n",
        "    \"windstorm\": {\"wind_ms\": 27.78},           # ~100 km/h -> kÃ¡ros vihar\n",
        "    \"heatwave\": {\"temp_C\": 35.0},              # napi kÃ¶zÃ©p > 35Â°C\n",
        "    \"coldwave\": {\"temp_C\": -10.0},             # napi kÃ¶zÃ©p < -10Â°C (pÃ©lda)\n",
        "    \"snowstorm\": {\"sd_m\": 0.2},                # hÃ³ > 20 cm\n",
        "    \"solaranomaly\": {\"ssrd_high\": None, \"ssrd_low\": None}  # Ã¡ltalÃ¡ban percentilis-alapÃº, fizikai kÃ¼szÃ¶b kevÃ©sbÃ© egyÃ©rtelmÅ±\n",
        "}\n",
        "\n",
        "# V0 lookup (ahogy megadtad)\n",
        "type_map = {\n",
        "    (\"wind\", \"offshore\"): 0.6, (\"wind\", \"onshore\"): 0.4,\n",
        "    (\"solar\", \"pv\"): 0.3, (\"hydro\", \"-\"): 0.5,\n",
        "    (\"biomass\", \"-\"): 0.4, (\"gas\", \"-\"): 0.7,\n",
        "    (\"coal\", \"-\"): 0.7, (\"nuclear\", \"-\"): 0.8,\n",
        "    (\"hydrogen\", \"-\"): 0.9, (\"storage\", \"battery\"): 0.7,\n",
        "    (\"grid\", \"transmission\"): 0.6,\n",
        "}\n",
        "\n",
        "def get_V0(row):\n",
        "    key = (str(row.get(\"type\",\"\")).strip().lower(), str(row.get(\"subtype\",\"\")).strip().lower())\n",
        "    return type_map.get(key, 0.5)\n",
        "\n",
        "# ---------- HELPER FUNKCIÃ“K ----------\n",
        "def detect_kelvin_and_to_c(series):\n",
        "    \"\"\"Ha Ã¡tlag >200 -> Kelvin, konvertÃ¡ljuk C-re\"\"\"\n",
        "    if series.dropna().empty:\n",
        "        return series\n",
        "    if series.mean() > 200:\n",
        "        return series - 273.15\n",
        "    return series\n",
        "\n",
        "def winsorize_series(s, lower_q=0.01, upper_q=0.99):\n",
        "    vals = s.dropna()\n",
        "    if vals.empty:\n",
        "        return s\n",
        "    lo = vals.quantile(lower_q)\n",
        "    hi = vals.quantile(upper_q)\n",
        "    return s.clip(lower=lo, upper=hi)\n",
        "\n",
        "def minmax_norm(s):\n",
        "    if s.dropna().empty:\n",
        "        return s\n",
        "    mn = s.min()\n",
        "    mx = s.max()\n",
        "    if np.isclose(mx, mn):\n",
        "        return s*0.0\n",
        "    return (s - mn) / (mx - mn)\n",
        "\n",
        "# ---------- 1) megtalÃ¡ljuk a daily_merged fÃ¡jlokat ----------\n",
        "daily_files = glob.glob(os.path.join(INPUT_DIR, \"*_daily_merged.csv\"))\n",
        "print(\"TalÃ¡lt daily_merged fÃ¡jlok:\", [os.path.basename(f) for f in daily_files])\n",
        "if not daily_files:\n",
        "    raise SystemExit(\"Nincs *_daily_merged.csv fÃ¡jl a /content mappÃ¡ban. TÃ¶ltsd fel Å‘ket!\")\n",
        "\n",
        "# ---------- 2) feldolgozzuk minden fÃ¡jlt region/Ã©v bontÃ¡sban ----------\n",
        "# aggregÃ¡ljuk a 6 hazard-ot region-year szinten\n",
        "hazard_rows = {\n",
        "    \"flood\": [], \"windstorm\": [], \"heatwave\": [], \"coldwave\": [], \"snowstorm\": [], \"solaranomaly\": []\n",
        "}\n",
        "\n",
        "for f in daily_files:\n",
        "    fname = os.path.basename(f)\n",
        "    region = fname.split(\"_daily_merged.csv\")[0].lower()\n",
        "    print(\"FeldolgozÃ¡s:\", region, \"->\", fname)\n",
        "    df = pd.read_csv(f)\n",
        "    # kis-nagybetÅ±s oszlopok\n",
        "    df.columns = [c.strip() for c in df.columns]\n",
        "    cols_low = {c.lower(): c for c in df.columns}\n",
        "    # kÃ¶telezÅ‘: date\n",
        "    if 'date' not in cols_low:\n",
        "        print(\"  !!! Nincs 'date' oszlop a\", fname, \"- fÃ¡jlban. Kihagyom.\")\n",
        "        continue\n",
        "    df['date'] = pd.to_datetime(df[cols_low['date']], errors='coerce')\n",
        "    df = df.dropna(subset=['date']).copy()\n",
        "    df['year'] = df['date'].dt.year\n",
        "\n",
        "    # standardizÃ¡lt oszlopvÃ¡ltozÃ³k lekÃ©rÃ©se (ha vannak)\n",
        "    def col(c):\n",
        "        return cols_low.get(c.lower(), None)\n",
        "\n",
        "    # elÅ‘kÃ©szÃ­tÃ©s: szÃ©lsebessÃ©g, t2m_C, stb.\n",
        "    # Wind speed: prefer u10_max_mean & v10_max_mean\n",
        "    if col('u10_max_mean') and col('v10_max_mean'):\n",
        "        u = df[col('u10_max_mean')].fillna(0.0)\n",
        "        v = df[col('v10_max_mean')].fillna(0.0)\n",
        "        df['wind_speed'] = np.sqrt(u*u + v*v)\n",
        "    # temperature to C\n",
        "    if col('t2m_mean_mean'):\n",
        "        tcol = col('t2m_mean_mean')\n",
        "        df['t2m_C'] = detect_kelvin_and_to_c(df[tcol].astype(float))\n",
        "    # snow depth in meters?\n",
        "    if col('sd_mean_mean'):\n",
        "        df['sd_m'] = df[col('sd_mean_mean')].astype(float)\n",
        "    # solar ssrd\n",
        "    if col('ssrd_mean_mean'):\n",
        "        df['ssrd'] = df[col('ssrd_mean_mean')].astype(float)\n",
        "    # precipitation\n",
        "    if col('tp_sum_mean'):\n",
        "        df['tp_mm'] = df[col('tp_sum_mean')].astype(float)\n",
        "    # sea surface temp (sst) if needed\n",
        "    if col('sst_mean_mean'):\n",
        "        df['sst'] = df[col('sst_mean_mean')].astype(float)\n",
        "\n",
        "    years = sorted(df['year'].unique())\n",
        "    for yr in years:\n",
        "        sub = df[df['year']==yr]\n",
        "        days = len(sub)\n",
        "        if days == 0:\n",
        "            continue\n",
        "\n",
        "        # --- FLOOD ---\n",
        "        if 'tp_mm' in sub.columns:\n",
        "            H_raw = sub['tp_mm'].max() if sub['tp_mm'].dropna().size>0 else np.nan\n",
        "            # P_raw - elsÅ‘dlegesen fizikai kÃ¼szÃ¶b (ABS_THRESH), ha nincs, hasznÃ¡lunk 95%-os kÃ¼szÃ¶b\n",
        "            phys = ABS_THRESH['flood'].get('tp_mm_day', None)\n",
        "            if phys is not None:\n",
        "                P_raw = (sub['tp_mm'] > phys).sum() / days\n",
        "            else:\n",
        "                thr95 = df['tp_mm'].dropna().quantile(0.95)\n",
        "                P_raw = (sub['tp_mm'] > thr95).sum() / days\n",
        "        else:\n",
        "            H_raw = np.nan; P_raw = np.nan\n",
        "        hazard_rows['flood'].append({'region':region,'year':int(yr),'H_raw':float(H_raw) if not pd.isna(H_raw) else np.nan,'P_raw':float(P_raw) if not pd.isna(P_raw) else np.nan})\n",
        "\n",
        "        # --- WINDSTORM ---\n",
        "        if 'wind_speed' in sub.columns:\n",
        "            H_raw = sub['wind_speed'].max() if sub['wind_speed'].dropna().size>0 else np.nan\n",
        "            phys = ABS_THRESH['windstorm'].get('wind_ms', None)\n",
        "            if phys is not None:\n",
        "                P_raw = (sub['wind_speed'] > phys).sum() / days\n",
        "            else:\n",
        "                thr95 = df['wind_speed'].dropna().quantile(0.95)\n",
        "                P_raw = (sub['wind_speed'] > thr95).sum() / days\n",
        "        else:\n",
        "            H_raw = np.nan; P_raw = np.nan\n",
        "        hazard_rows['windstorm'].append({'region':region,'year':int(yr),'H_raw':float(H_raw) if not pd.isna(H_raw) else np.nan,'P_raw':float(P_raw) if not pd.isna(P_raw) else np.nan})\n",
        "\n",
        "        # --- HEATWAVE ---\n",
        "        if 't2m_C' in sub.columns:\n",
        "            H_raw = sub['t2m_C'].max() if sub['t2m_C'].dropna().size>0 else np.nan\n",
        "            phys = ABS_THRESH['heatwave'].get('temp_C', None)\n",
        "            if phys is not None:\n",
        "                P_raw = (sub['t2m_C'] > phys).sum() / days\n",
        "            else:\n",
        "                thr95 = df['t2m_C'].dropna().quantile(0.95)\n",
        "                P_raw = (sub['t2m_C'] > thr95).sum() / days\n",
        "        else:\n",
        "            H_raw = np.nan; P_raw = np.nan\n",
        "        hazard_rows['heatwave'].append({'region':region,'year':int(yr),'H_raw':float(H_raw) if not pd.isna(H_raw) else np.nan,'P_raw':float(P_raw) if not pd.isna(P_raw) else np.nan})\n",
        "\n",
        "        # --- COLDWAVE (extrÃ©m hideg) ---\n",
        "        if 't2m_C' in sub.columns:\n",
        "            H_raw = sub['t2m_C'].min() if sub['t2m_C'].dropna().size>0 else np.nan\n",
        "            phys = ABS_THRESH['coldwave'].get('temp_C', None)\n",
        "            if phys is not None:\n",
        "                P_raw = (sub['t2m_C'] < phys).sum() / days\n",
        "            else:\n",
        "                thr05 = df['t2m_C'].dropna().quantile(0.05)\n",
        "                P_raw = (sub['t2m_C'] < thr05).sum() / days\n",
        "        else:\n",
        "            H_raw = np.nan; P_raw = np.nan\n",
        "        # For H_raw for coldwave we store the absolute cold severity (negative allowed)\n",
        "        hazard_rows['coldwave'].append({'region':region,'year':int(yr),'H_raw':float(H_raw) if not pd.isna(H_raw) else np.nan,'P_raw':float(P_raw) if not pd.isna(P_raw) else np.nan})\n",
        "\n",
        "        # --- SNOWSTORM ---\n",
        "        if 'sd_m' in sub.columns:\n",
        "            H_raw = sub['sd_m'].max() if sub['sd_m'].dropna().size>0 else np.nan\n",
        "            phys = ABS_THRESH['snowstorm'].get('sd_m', None)\n",
        "            if phys is not None:\n",
        "                P_raw = (sub['sd_m'] > phys).sum() / days\n",
        "            else:\n",
        "                thr95 = df['sd_m'].dropna().quantile(0.95)\n",
        "                P_raw = (sub['sd_m'] > thr95).sum() / days\n",
        "        else:\n",
        "            H_raw = np.nan; P_raw = np.nan\n",
        "        hazard_rows['snowstorm'].append({'region':region,'year':int(yr),'H_raw':float(H_raw) if not pd.isna(H_raw) else np.nan,'P_raw':float(P_raw) if not pd.isna(P_raw) else np.nan})\n",
        "\n",
        "        # --- SOLAR ANOMALY (mindkÃ©t irÃ¡ny) ---\n",
        "        if 'ssrd' in sub.columns:\n",
        "            # H_raw: Ã©ves abs max eltÃ©rÃ©s a kÃ¶zÃ©pÃ©rtÃ©ktÅ‘l (szigetelt jelzÃ©s)\n",
        "            mean_ssrd = df['ssrd'].dropna().mean() if df['ssrd'].dropna().size>0 else 0.0\n",
        "            H_raw = (sub['ssrd'] - mean_ssrd).abs().max() if sub['ssrd'].dropna().size>0 else np.nan\n",
        "            # P_raw: ha van fizikai kÃ¼szÃ¶b nemzetkÃ¶zi szinten, hasznÃ¡ljuk. EgyÃ©bkÃ©nt percentilis kÃ©toldalÃºan.\n",
        "            thr_high = df['ssrd'].dropna().quantile(0.95) if df['ssrd'].dropna().size>0 else np.nan\n",
        "            thr_low = df['ssrd'].dropna().quantile(0.05) if df['ssrd'].dropna().size>0 else np.nan\n",
        "            P_raw = ((sub['ssrd'] > thr_high).sum() + (sub['ssrd'] < thr_low).sum()) / days\n",
        "        else:\n",
        "            H_raw = np.nan; P_raw = np.nan\n",
        "        hazard_rows['solaranomaly'].append({'region':region,'year':int(yr),'H_raw':float(H_raw) if not pd.isna(H_raw) else np.nan,'P_raw':float(P_raw) if not pd.isna(P_raw) else np.nan})\n",
        "\n",
        "    print(f\"  -> kÃ©sz: {region}, Ã©vek: {len(years)}\")\n",
        "\n",
        "# ---------- 3) winsorize Ã©s normalizÃ¡lÃ¡s regiononkÃ©nt, majd mentÃ©s ----------\n",
        "def postprocess_hazard(rows, hazard_name):\n",
        "    dfh = pd.DataFrame(rows)\n",
        "    if dfh.empty:\n",
        "        print(\"!! Ãœres:\", hazard_name)\n",
        "        return dfh\n",
        "    out_list = []\n",
        "    for region, g in dfh.groupby('region'):\n",
        "        gr = g.copy().sort_values('year').reset_index(drop=True)\n",
        "        # winsorize raw values (H_raw, P_raw) 1-99 percentile\n",
        "        for col in ['H_raw','P_raw']:\n",
        "            if col in gr.columns:\n",
        "                gr[col + \"_win\"] = winsorize_series(gr[col])\n",
        "        # normalize (min-max) on the winsorized values per region\n",
        "        if 'H_raw_win' in gr.columns:\n",
        "            gr['H_norm'] = minmax_norm(gr['H_raw_win'])\n",
        "        if 'P_raw_win' in gr.columns:\n",
        "            gr['P_norm'] = minmax_norm(gr['P_raw_win'])\n",
        "        out_list.append(gr)\n",
        "    df_out = pd.concat(out_list, ignore_index=True)\n",
        "    # select and reorder\n",
        "    cols_keep = [c for c in ['region','year','H_raw','P_raw','H_raw_win','P_raw_win','H_norm','P_norm'] if c in df_out.columns]\n",
        "    df_out = df_out[cols_keep]\n",
        "    outpath = os.path.join(OUTPUT_DIR, f\"hazard_yearly_{hazard_name}.csv\")\n",
        "    df_out.to_csv(outpath, index=False)\n",
        "    print(f\"âœ… Mentve: {outpath} (sor: {len(df_out)})\")\n",
        "    return df_out\n",
        "\n",
        "hz_dfs = {}\n",
        "for hz in hazard_rows.keys():\n",
        "    hz_dfs[hz] = postprocess_hazard(hazard_rows[hz], hz)\n",
        "\n",
        "# ---------- 4) E Ã©s V szÃ¡mÃ­tÃ¡sa, r kiszÃ¡mÃ­tÃ¡sa ----------\n",
        "# E: (H_norm + P_norm)/2 * region_weight  (region_weight a region string alapjÃ¡n)\n",
        "# V: assets alapjÃ¡n\n",
        "if not os.path.exists(ASSETS_PATH):\n",
        "    raise SystemExit(\"assets.xlsx nincs a megadott helyen! TÃ¶ltsd fel a /content/assets.xlsx fÃ¡jlba.\")\n",
        "\n",
        "assets = pd.read_excel(ASSETS_PATH, dtype=str)\n",
        "assets.columns = [c.strip() for c in assets.columns]\n",
        "# biztosÃ­tsuk a kÃ¶telezÅ‘ oszlopokat Ã©s tÃ­pusokat\n",
        "if 'asset_id' not in assets.columns:\n",
        "    raise SystemExit(\"assets.xlsx nem tartalmaz 'asset_id' oszlopot.\")\n",
        "for col in ['capacity_MW','commission_year','latitude','longitude']:\n",
        "    if col in assets.columns:\n",
        "        assets[col] = pd.to_numeric(assets[col], errors='coerce')\n",
        "\n",
        "# normalizÃ¡ljuk az orszÃ¡gneveket egyszerÅ± string->region mappingre (ha szÃ¼ksÃ©ges)\n",
        "def normalize_country_to_region(c):\n",
        "    if pd.isna(c): return np.nan\n",
        "    s = str(c).strip().lower()\n",
        "    mapping = {\n",
        "        'denmark':'dania','denmark ':'dania','dk':'dania',\n",
        "        'germany':'nemet','germany ':'nemet','de':'nemet',\n",
        "        'united kingdom':'uk','uk':'uk','england':'uk','gb':'uk',\n",
        "        'netherlands':'netherlands','nl':'netherlands',\n",
        "        'taiwan':'tajvan','tw':'tajvan',\n",
        "        'usa':'usa','united states':'usa','us':'usa','america':'usa'\n",
        "    }\n",
        "    return mapping.get(s, s)\n",
        "\n",
        "assets['region_norm'] = assets['country'].apply(normalize_country_to_region)\n",
        "\n",
        "# V szÃ¡mÃ­tÃ¡sa (V0 + age + maintenance)\n",
        "assets['V0'] = assets.apply(get_V0, axis=1)\n",
        "assets['commission_year'] = pd.to_numeric(assets.get('commission_year', pd.Series([np.nan]*len(assets))), errors='coerce')\n",
        "assets['age_factor'] = 2025 - assets['commission_year'].fillna(2025)\n",
        "assets['maintenance'] = 0.7\n",
        "# a formula, amit adtÃ¡l: V=clip(V0 * (1+0.3*age_factor) * (1-0.4*maintenance),0,1)\n",
        "assets['V'] = np.clip(assets['V0'] * (1 + 0.3 * (assets['age_factor']) ) * (1 - 0.4 * assets['maintenance']), 0, 1)\n",
        "\n",
        "assets[['asset_id','region_norm','V0','age_factor','maintenance','V']].to_csv(os.path.join(OUTPUT_DIR,'vulnerability_matrix.csv'), index=False)\n",
        "print(\"âœ… vulnerability_matrix mentve\")\n",
        "\n",
        "# Ã¶sszeÃ¡llÃ­tjuk az r sorokat: minden hazard df-t pÃ¡rosÃ­tunk az assets regionnel\n",
        "r_rows = []\n",
        "for hz, dfhz in hz_dfs.items():\n",
        "    if dfhz is None or dfhz.empty:\n",
        "        continue\n",
        "    for _, a in assets.iterrows():\n",
        "        region = a['region_norm']\n",
        "        subset = dfhz[dfhz['region'] == region]\n",
        "        if subset.empty:\n",
        "            # nincs hazard adat erre a rÃ©giÃ³ra -> kihagyjuk\n",
        "            continue\n",
        "        for _, s in subset.iterrows():\n",
        "            Hn = s.get('H_norm', np.nan)\n",
        "            Pn = s.get('P_norm', np.nan)\n",
        "            E = ((s.get('H_norm', np.nan) + s.get('P_norm', np.nan))/2.0) * region_weights.get(region, 0.8) if (not pd.isna(s.get('H_norm')) and not pd.isna(s.get('P_norm'))) else np.nan\n",
        "            V = a.get('V', np.nan)\n",
        "            r_val = Hn * Pn * E * V if (not pd.isna(Hn) and not pd.isna(Pn) and not pd.isna(E) and not pd.isna(V)) else np.nan\n",
        "            r_rows.append({\n",
        "                'asset_id': a['asset_id'],\n",
        "                'hazard': hz,\n",
        "                'region': region,\n",
        "                'year': int(s['year']),\n",
        "                'H_norm': Hn,\n",
        "                'P_norm': Pn,\n",
        "                'E': E,\n",
        "                'V': V,\n",
        "                'r': r_val\n",
        "            })\n",
        "\n",
        "df_r = pd.DataFrame(r_rows)\n",
        "out_r = os.path.join(OUTPUT_DIR, \"r_values_enhanced.csv\")\n",
        "df_r.to_csv(out_r, index=False)\n",
        "print(f\"âœ… Mentve r_values_enhanced: {out_r}  (sorok: {len(df_r)})\")\n",
        "\n",
        "# ---------- 5) rÃ¶vid Ã¶sszegzÃ©s ----------\n",
        "print(\"\\n--- Ã–SSZEGZÃ‰S ---\")\n",
        "for hz in hz_dfs:\n",
        "    dfhz = hz_dfs[hz]\n",
        "    print(f\"{hz}: fÃ¡jl sorok: {len(dfhz) if dfhz is not None else 0}\")\n",
        "print(\"assets:\", len(assets))\n",
        "print(\"r sorok:\", len(df_r))\n",
        "print(\"KÃ©szen. EllenÅ‘rizd a mappÃ¡t:\", OUTPUT_DIR)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gHgyEEbvS5wn",
        "outputId": "54a107fc-8571-43fc-a369-a59de88b0efb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TalÃ¡lt daily_merged fÃ¡jlok: ['usa_daily_merged.csv', 'nemet_daily_merged.csv', 'tajvan_daily_merged.csv', 'dania_daily_merged.csv', 'uk_daily_merged.csv']\n",
            "FeldolgozÃ¡s: usa -> usa_daily_merged.csv\n",
            "  -> kÃ©sz: usa, Ã©vek: 24\n",
            "FeldolgozÃ¡s: nemet -> nemet_daily_merged.csv\n",
            "  -> kÃ©sz: nemet, Ã©vek: 24\n",
            "FeldolgozÃ¡s: tajvan -> tajvan_daily_merged.csv\n",
            "  -> kÃ©sz: tajvan, Ã©vek: 24\n",
            "FeldolgozÃ¡s: dania -> dania_daily_merged.csv\n",
            "  -> kÃ©sz: dania, Ã©vek: 24\n",
            "FeldolgozÃ¡s: uk -> uk_daily_merged.csv\n",
            "  -> kÃ©sz: uk, Ã©vek: 24\n",
            "âœ… Mentve: /content/cpri_outputs/hazard_yearly_flood.csv (sor: 120)\n",
            "âœ… Mentve: /content/cpri_outputs/hazard_yearly_windstorm.csv (sor: 120)\n",
            "âœ… Mentve: /content/cpri_outputs/hazard_yearly_heatwave.csv (sor: 120)\n",
            "âœ… Mentve: /content/cpri_outputs/hazard_yearly_coldwave.csv (sor: 120)\n",
            "âœ… Mentve: /content/cpri_outputs/hazard_yearly_snowstorm.csv (sor: 120)\n",
            "âœ… Mentve: /content/cpri_outputs/hazard_yearly_solaranomaly.csv (sor: 120)\n",
            "âœ… vulnerability_matrix mentve\n",
            "âœ… Mentve r_values_enhanced: /content/cpri_outputs/r_values_enhanced.csv  (sorok: 3600)\n",
            "\n",
            "--- Ã–SSZEGZÃ‰S ---\n",
            "flood: fÃ¡jl sorok: 120\n",
            "windstorm: fÃ¡jl sorok: 120\n",
            "heatwave: fÃ¡jl sorok: 120\n",
            "coldwave: fÃ¡jl sorok: 120\n",
            "snowstorm: fÃ¡jl sorok: 120\n",
            "solaranomaly: fÃ¡jl sorok: 120\n",
            "assets: 26\n",
            "r sorok: 3600\n",
            "KÃ©szen. EllenÅ‘rizd a mappÃ¡t: /content/cpri_outputs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# ðŸŒ Ã˜rsted â€“ CPRI index: teljes pipeline\n",
        "# LÃ©pÃ©sek:\n",
        "# 1ï¸âƒ£ MICE imputÃ¡ciÃ³ (daily_merged â†’ daily_imputed)\n",
        "# 2ï¸âƒ£ Ã‰ves hazardok szÃ¡mÃ­tÃ¡sa\n",
        "# 3ï¸âƒ£ Exposure (E) Ã©s Vulnerability (V)\n",
        "# 4ï¸âƒ£ r_values_enhanced.csv mentÃ©se\n",
        "# ============================================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "\n",
        "# --- PARAMÃ‰TEREK ---\n",
        "input_dir = \"/content/cpri_outputs\"\n",
        "os.makedirs(input_dir, exist_ok=True)\n",
        "\n",
        "region_weights = {\n",
        "    \"dania\": 0.8, \"denmark\": 0.8,\n",
        "    \"nemet\": 0.6, \"germany\": 0.6,\n",
        "    \"uk\": 0.9, \"netherlands\": 0.9,\n",
        "    \"usa\": 1.0, \"united states\": 1.0,\n",
        "    \"tajvan\": 1.0, \"taiwan\": 1.0\n",
        "}\n",
        "\n",
        "# Vulnerability alapÃ©rtÃ©kek\n",
        "type_map = {\n",
        "    (\"wind\", \"offshore\"): 0.6, (\"wind\", \"onshore\"): 0.4,\n",
        "    (\"solar\", \"pv\"): 0.3, (\"hydro\", \"-\"): 0.5,\n",
        "    (\"biomass\", \"-\"): 0.4, (\"gas\", \"-\"): 0.7,\n",
        "    (\"coal\", \"-\"): 0.7, (\"nuclear\", \"-\"): 0.8,\n",
        "    (\"hydrogen\", \"-\"): 0.9, (\"storage\", \"battery\"): 0.7,\n",
        "    (\"grid\", \"transmission\"): 0.6,\n",
        "}\n",
        "\n",
        "# --- 1ï¸âƒ£ MICE IMPUTÃCIÃ“ ---\n",
        "print(\"ðŸ”§ MICE imputÃ¡ciÃ³ indul...\")\n",
        "daily_files = [f for f in os.listdir(input_dir) if f.endswith(\"_daily_merged.csv\")]\n",
        "imputer = IterativeImputer(random_state=42, max_iter=10)\n",
        "\n",
        "for f in daily_files:\n",
        "    path = os.path.join(input_dir, f)\n",
        "    df = pd.read_csv(path)\n",
        "    num = df.select_dtypes(include=[np.number])\n",
        "    if num.empty:\n",
        "        continue\n",
        "    num_imputed = imputer.fit_transform(num)\n",
        "    df[num.columns] = num_imputed\n",
        "    out_path = path.replace(\"_merged.csv\", \"_imputed.csv\")\n",
        "    df.to_csv(out_path, index=False)\n",
        "    print(f\"âœ… ImputÃ¡lt: {out_path} ({len(df)} sor)\")\n",
        "\n",
        "# --- 2ï¸âƒ£ HAZARD DERIVÃLÃS ---\n",
        "print(\"\\nðŸŒª Hazard derivÃ¡lÃ¡s folyamatban...\")\n",
        "hazards = [\"flood\", \"windstorm\", \"heatwave\", \"coldwave\", \"snowstorm\", \"solar_anomaly\"]\n",
        "hazard_dfs = []\n",
        "\n",
        "for f in os.listdir(input_dir):\n",
        "    if not f.endswith(\"_daily_imputed.csv\"):\n",
        "        continue\n",
        "\n",
        "    region = f.split(\"_\")[0].lower()\n",
        "    df = pd.read_csv(os.path.join(input_dir, f))\n",
        "    df[\"year\"] = pd.to_datetime(df[\"date\"]).dt.year\n",
        "    grouped = df.groupby(\"year\").agg(\"mean\").reset_index()\n",
        "\n",
        "    # --- H_raw Ã©s P_raw szÃ¡mÃ­tÃ¡s ---\n",
        "    # P_raw = extrÃ©m esemÃ©ny valÃ³szÃ­nÅ±sÃ©ge (95. percentil feletti napok arÃ¡nya)\n",
        "    def compute_P_raw(series):\n",
        "        threshold = np.nanpercentile(series, 95)\n",
        "        return np.mean(series > threshold)\n",
        "\n",
        "    h_vars = {\n",
        "        \"flood\": \"tp_sum_mean\",\n",
        "        \"windstorm\": [\"u10_max_mean\", \"v10_max_mean\"],\n",
        "        \"heatwave\": \"t2m_mean_mean\",\n",
        "        \"coldwave\": \"t2m_mean_mean\",\n",
        "        \"snowstorm\": \"sd_mean_mean\",\n",
        "        \"solar_anomaly\": \"ssrd_mean_mean\"\n",
        "    }\n",
        "\n",
        "    for hz, cols in h_vars.items():\n",
        "        df_h = grouped.copy()\n",
        "        if isinstance(cols, list):\n",
        "            df_h[\"H_raw\"] = np.sqrt(df[cols[0]]**2 + df[cols[1]]**2)\n",
        "        else:\n",
        "            df_h[\"H_raw\"] = df[cols]\n",
        "\n",
        "        df_h[\"P_raw\"] = compute_P_raw(df_h[\"H_raw\"])\n",
        "\n",
        "        # NormÃ¡lÃ¡s 0â€“1 kÃ¶zÃ©\n",
        "        df_h[\"H_norm\"] = (df_h[\"H_raw\"] - df_h[\"H_raw\"].min()) / (df_h[\"H_raw\"].max() - df_h[\"H_raw\"].min() + 1e-9)\n",
        "        df_h[\"P_norm\"] = (df_h[\"P_raw\"] - df_h[\"P_raw\"].min()) / (df_h[\"P_raw\"].max() - df_h[\"P_raw\"].min() + 1e-9)\n",
        "\n",
        "        df_h[\"hazard\"] = hz\n",
        "        df_h[\"region\"] = region\n",
        "        df_h[\"region_weight\"] = region_weights.get(region, 1.0)\n",
        "        df_h[\"E\"] = (df_h[\"H_norm\"] + df_h[\"P_norm\"]**2) * df_h[\"region_weight\"]\n",
        "\n",
        "        hazard_dfs.append(df_h[[\"hazard\", \"region\", \"year\", \"H_norm\", \"P_norm\", \"E\"]])\n",
        "\n",
        "hazard_all = pd.concat(hazard_dfs, ignore_index=True)\n",
        "hazard_all.to_csv(f\"{input_dir}/exposure_matrix.csv\", index=False)\n",
        "print(f\"âœ… Exposure mentve ({len(hazard_all)} sor)\")\n",
        "\n",
        "# --- 3ï¸âƒ£ VULNERABILITY ---\n",
        "print(\"\\nðŸ§© Vulnerability szÃ¡mÃ­tÃ¡s...\")\n",
        "assets = pd.read_excel(f\"{input_dir}/assets.xlsx\")\n",
        "\n",
        "def get_V0(row):\n",
        "    key = (row[\"type\"].lower(), str(row[\"subtype\"]).lower() if pd.notna(row[\"subtype\"]) else \"-\")\n",
        "    return type_map.get(key, 0.5)\n",
        "\n",
        "assets[\"V0\"] = assets.apply(get_V0, axis=1)\n",
        "assets[\"age_factor\"] = 2025 - assets[\"commission_year\"]\n",
        "assets[\"maintenance\"] = 0.7\n",
        "assets[\"V\"] = np.clip(\n",
        "    assets[\"V0\"] * (1 + 0.3 * assets[\"age_factor\"] / 50) * (1 - 0.4 * assets[\"maintenance\"]),\n",
        "    0, 1\n",
        ")\n",
        "assets.to_csv(f\"{input_dir}/vulnerability_matrix.csv\", index=False)\n",
        "print(f\"âœ… Vulnerability mentve ({len(assets)} asset)\")\n",
        "\n",
        "# --- 4ï¸âƒ£ r VALUES ENHANCED ---\n",
        "print(\"\\nðŸ§® r Ã©rtÃ©kek szÃ¡mÃ­tÃ¡sa...\")\n",
        "merged = []\n",
        "for _, hz_row in hazard_all.iterrows():\n",
        "    region = hz_row[\"region\"]\n",
        "    hz = hz_row[\"hazard\"]\n",
        "    year = hz_row[\"year\"]\n",
        "    subset = assets[assets[\"country\"].str.lower().str.contains(region[:2], na=False)]\n",
        "    for _, a in subset.iterrows():\n",
        "        r_val = hz_row[\"H_norm\"] * hz_row[\"P_norm\"] * hz_row[\"E\"] * a[\"V\"]\n",
        "        merged.append({\n",
        "            \"asset_id\": a[\"asset_id\"],\n",
        "            \"hazard\": hz,\n",
        "            \"region\": region,\n",
        "            \"year\": year,\n",
        "            \"H_norm\": hz_row[\"H_norm\"],\n",
        "            \"P_norm\": hz_row[\"P_norm\"],\n",
        "            \"E\": hz_row[\"E\"],\n",
        "            \"V\": a[\"V\"],\n",
        "            \"r\": r_val\n",
        "        })\n",
        "\n",
        "r_df = pd.DataFrame(merged)\n",
        "r_df.to_csv(f\"{input_dir}/r_values_enhanced.csv\", index=False)\n",
        "print(f\"âœ… Mentve: {input_dir}/r_values_enhanced.csv ({len(r_df)} sor)\")\n",
        "\n",
        "print(\"\\nðŸŽ¯ KÃ‰SZ â€“ Minden lÃ©pÃ©s sikeresen lefutott.\")\n"
      ],
      "metadata": {
        "id": "LIz-TAyEoi1F",
        "outputId": "6215ffcf-d751-4e8a-9f6f-8a1aaed64aec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 911
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ”§ MICE imputÃ¡ciÃ³ indul...\n",
            "âœ… ImputÃ¡lt: /content/cpri_outputs/usa_daily_imputed.csv (8766 sor)\n",
            "âœ… ImputÃ¡lt: /content/cpri_outputs/nemet_daily_imputed.csv (8766 sor)\n",
            "âœ… ImputÃ¡lt: /content/cpri_outputs/tajvan_daily_imputed.csv (8766 sor)\n",
            "âœ… ImputÃ¡lt: /content/cpri_outputs/dania_daily_imputed.csv (8766 sor)\n",
            "âœ… ImputÃ¡lt: /content/cpri_outputs/uk_daily_imputed.csv (8766 sor)\n",
            "\n",
            "ðŸŒª Hazard derivÃ¡lÃ¡s folyamatban...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "agg function failed [how->mean,dtype->object]",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36m_agg_py_fallback\u001b[0;34m(self, how, values, ndim, alt)\u001b[0m\n\u001b[1;32m   1941\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1942\u001b[0;31m             \u001b[0mres_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grouper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg_series\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreserve_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1943\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/groupby/ops.py\u001b[0m in \u001b[0;36magg_series\u001b[0;34m(self, obj, func, preserve_dtype)\u001b[0m\n\u001b[1;32m    863\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 864\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_aggregate_series_pure_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/groupby/ops.py\u001b[0m in \u001b[0;36m_aggregate_series_pure_python\u001b[0;34m(self, obj, func)\u001b[0m\n\u001b[1;32m    884\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplitter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   2453\u001b[0m                 \u001b[0;34m\"mean\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2454\u001b[0;31m                 \u001b[0malt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumeric_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnumeric_only\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2455\u001b[0m                 \u001b[0mnumeric_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnumeric_only\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mmean\u001b[0;34m(self, axis, skipna, numeric_only, **kwargs)\u001b[0m\n\u001b[1;32m   6548\u001b[0m     ):\n\u001b[0;32m-> 6549\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mNDFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumeric_only\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mmean\u001b[0;34m(self, axis, skipna, numeric_only, **kwargs)\u001b[0m\n\u001b[1;32m  12419\u001b[0m     ) -> Series | float:\n\u001b[0;32m> 12420\u001b[0;31m         return self._stat_function(\n\u001b[0m\u001b[1;32m  12421\u001b[0m             \u001b[0;34m\"mean\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnanops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnanmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumeric_only\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_stat_function\u001b[0;34m(self, name, func, axis, skipna, numeric_only, **kwargs)\u001b[0m\n\u001b[1;32m  12376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m> 12377\u001b[0;31m         return self._reduce(\n\u001b[0m\u001b[1;32m  12378\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskipna\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumeric_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnumeric_only\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_reduce\u001b[0;34m(self, op, name, axis, skipna, numeric_only, filter_type, **kwds)\u001b[0m\n\u001b[1;32m   6456\u001b[0m                 )\n\u001b[0;32m-> 6457\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelegate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskipna\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/nanops.py\u001b[0m in \u001b[0;36mf\u001b[0;34m(values, axis, skipna, **kwds)\u001b[0m\n\u001b[1;32m    146\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskipna\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/nanops.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(values, axis, skipna, mask, **kwargs)\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 404\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskipna\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/nanops.py\u001b[0m in \u001b[0;36mnanmean\u001b[0;34m(values, axis, skipna, mask)\u001b[0m\n\u001b[1;32m    719\u001b[0m     \u001b[0mthe_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype_sum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 720\u001b[0;31m     \u001b[0mthe_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ensure_numeric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthe_sum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    721\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/nanops.py\u001b[0m in \u001b[0;36m_ensure_numeric\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1700\u001b[0m             \u001b[0;31m# GH#44008, GH#36703 avoid casting e.g. strings to numeric\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1701\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Could not convert string '{x}' to numeric\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1702\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Could not convert string '2000-01-012000-01-022000-01-032000-01-042000-01-052000-01-062000-01-072000-01-082000-01-092000-01-102000-01-112000-01-122000-01-132000-01-142000-01-152000-01-162000-01-172000-01-182000-01-192000-01-202000-01-212000-01-222000-01-232000-01-242000-01-252000-01-262000-01-272000-01-282000-01-292000-01-302000-01-312000-02-012000-02-022000-02-032000-02-042000-02-052000-02-062000-02-072000-02-082000-02-092000-02-102000-02-112000-02-122000-02-132000-02-142000-02-152000-02-162000-02-172000-02-182000-02-192000-02-202000-02-212000-02-222000-02-232000-02-242000-02-252000-02-262000-02-272000-02-282000-02-292000-03-012000-03-022000-03-032000-03-042000-03-052000-03-062000-03-072000-03-082000-03-092000-03-102000-03-112000-03-122000-03-132000-03-142000-03-152000-03-162000-03-172000-03-182000-03-192000-03-202000-03-212000-03-222000-03-232000-03-242000-03-252000-03-262000-03-272000-03-282000-03-292000-03-302000-03-312000-04-012000-04-022000-04-032000-04-042000-04-052000-04-062000-04-072000-04-082000-04-092000-04-102000-04-112000-04-122000-04-132000-04-142000-04-152000-04-162000-04-172000-04-182000-04-192000-04-202000-04-212000-04-222000-04-232000-04-242000-04-252000-04-262000-04-272000-04-282000-04-292000-04-302000-05-012000-05-022000-05-032000-05-042000-05-052000-05-062000-05-072000-05-082000-05-092000-05-102000-05-112000-05-122000-05-132000-05-142000-05-152000-05-162000-05-172000-05-182000-05-192000-05-202000-05-212000-05-222000-05-232000-05-2...",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-350727308.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"year\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"date\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myear\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0mgrouped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"year\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mean\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;31m# --- H_raw Ã©s P_raw szÃ¡mÃ­tÃ¡s ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/groupby/generic.py\u001b[0m in \u001b[0;36maggregate\u001b[0;34m(self, func, engine, engine_kwargs, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1431\u001b[0m         \u001b[0mop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGroupByApply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1432\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_dict_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1434\u001b[0m             \u001b[0;31m# GH #52849\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36magg\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_dict_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_str\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    601\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"axis\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 603\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_list_or_dict_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mDataFrame\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36m_apply_str\u001b[0;34m(self, obj, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    691\u001b[0m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 693\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m             \u001b[0;31m# people may aggregate on a non-callable attribute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36mmean\u001b[0;34m(self, numeric_only, engine, engine_kwargs)\u001b[0m\n\u001b[1;32m   2450\u001b[0m             )\n\u001b[1;32m   2451\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2452\u001b[0;31m             result = self._cython_agg_general(\n\u001b[0m\u001b[1;32m   2453\u001b[0m                 \u001b[0;34m\"mean\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2454\u001b[0m                 \u001b[0malt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumeric_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnumeric_only\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36m_cython_agg_general\u001b[0;34m(self, how, alt, numeric_only, min_count, **kwargs)\u001b[0m\n\u001b[1;32m   1996\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1997\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1998\u001b[0;31m         \u001b[0mnew_mgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrouped_reduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1999\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrap_agged_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_mgr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2000\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"idxmin\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"idxmax\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mgrouped_reduce\u001b[0;34m(self, func)\u001b[0m\n\u001b[1;32m   1467\u001b[0m                 \u001b[0;31m#  while others do not.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1468\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0msb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mblk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1469\u001b[0;31m                     \u001b[0mapplied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1470\u001b[0m                     \u001b[0mresult_blocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextend_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapplied\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_blocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1471\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, **kwargs)\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[0mone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m         \"\"\"\n\u001b[0;32m--> 393\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaybe_coerce_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36marray_func\u001b[0;34m(values)\u001b[0m\n\u001b[1;32m   1993\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1994\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0malt\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1995\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_agg_py_fallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1996\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1997\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36m_agg_py_fallback\u001b[0;34m(self, how, values, ndim, alt)\u001b[0m\n\u001b[1;32m   1944\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"agg function failed [how->{how},dtype->{ser.dtype}]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1945\u001b[0m             \u001b[0;31m# preserve the kind of exception that raised\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1946\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1948\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: agg function failed [how->mean,dtype->object]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xrmxjmDGG8wK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "import pandas as pd, numpy as np, os\n",
        "\n",
        "input_dir = \"/content/cpri_outputs\"\n",
        "imputer = IterativeImputer(random_state=42, max_iter=10)\n",
        "\n",
        "for f in os.listdir(input_dir):\n",
        "    if f.endswith(\"_daily_merged.csv\"):\n",
        "        df = pd.read_csv(f\"{input_dir}/{f}\")\n",
        "        num = df.select_dtypes(include=[np.number])\n",
        "        if not num.empty:\n",
        "            num_imputed = imputer.fit_transform(num)\n",
        "            df[num.columns] = num_imputed\n",
        "        df.to_csv(f\"{input_dir}/{f.replace('_merged.csv', '_imputed.csv')}\", index=False)\n",
        "        print(f\"âœ… {f} â†’ imputÃ¡lt fÃ¡jl mentve.\")\n"
      ],
      "metadata": {
        "id": "rRsrtys1e77J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a656a1a-70a4-4442-954d-d7fb825f5622"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… usa_daily_merged.csv â†’ imputÃ¡lt fÃ¡jl mentve.\n",
            "âœ… nemet_daily_merged.csv â†’ imputÃ¡lt fÃ¡jl mentve.\n",
            "âœ… tajvan_daily_merged.csv â†’ imputÃ¡lt fÃ¡jl mentve.\n",
            "âœ… dania_daily_merged.csv â†’ imputÃ¡lt fÃ¡jl mentve.\n",
            "âœ… uk_daily_merged.csv â†’ imputÃ¡lt fÃ¡jl mentve.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "\n",
        "# --- MappÃ¡k ---\n",
        "input_dir = \"/content/cpri_outputs\"\n",
        "os.makedirs(input_dir, exist_ok=True)\n",
        "\n",
        "# --- MICE imputÃ¡lÃ³ elÅ‘kÃ©szÃ­tÃ©se ---\n",
        "imputer = IterativeImputer(random_state=42, max_iter=10, sample_posterior=True)\n",
        "\n",
        "# --- FÃ¡jlok keresÃ©se ---\n",
        "files = [f for f in os.listdir(input_dir) if f.endswith(\"_daily_merged.csv\")]\n",
        "if not files:\n",
        "    print(\"âŒ Nincsenek *_daily_merged.csv fÃ¡jlok a cpri_outputs mappÃ¡ban!\")\n",
        "else:\n",
        "    print(f\"ðŸ“‚ TalÃ¡lt fÃ¡jlok: {files}\")\n",
        "\n",
        "# --- Minden fÃ¡jl feldolgozÃ¡sa ---\n",
        "for f in files:\n",
        "    file_path = os.path.join(input_dir, f)\n",
        "    try:\n",
        "        print(f\"\\nðŸ”§ FeldolgozÃ¡s: {f}\")\n",
        "        df = pd.read_csv(file_path)\n",
        "\n",
        "        # Csak numerikus oszlopok\n",
        "        num_cols = df.select_dtypes(include=[np.number]).columns\n",
        "        if len(num_cols) == 0:\n",
        "            print(f\"âš ï¸ Nincs numerikus adat: {f}\")\n",
        "            continue\n",
        "\n",
        "        # MICE imputÃ¡lÃ¡s\n",
        "        imputed = imputer.fit_transform(df[num_cols])\n",
        "        df[num_cols] = imputed\n",
        "\n",
        "        out_path = os.path.join(input_dir, f.replace(\"_merged.csv\", \"_imputed.csv\"))\n",
        "        df.to_csv(out_path, index=False)\n",
        "        print(f\"âœ… ImputÃ¡lt fÃ¡jl mentve: {out_path}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"ðŸ’€ Hiba a {f} fÃ¡jlnÃ¡l: {e}\")\n",
        "\n",
        "print(\"\\nðŸ KÃ‰SZ â€” nÃ©zd meg a cpri_outputs mappÃ¡t, ott lesznek az Ãºj *_daily_imputed.csv fÃ¡jlok.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H3LomjLfhGgf",
        "outputId": "9267e188-4fc1-42d7-a59f-5b1030b3023a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“‚ TalÃ¡lt fÃ¡jlok: ['usa_daily_merged.csv', 'nemet_daily_merged.csv', 'tajvan_daily_merged.csv', 'dania_daily_merged.csv', 'uk_daily_merged.csv']\n",
            "\n",
            "ðŸ”§ FeldolgozÃ¡s: usa_daily_merged.csv\n",
            "âœ… ImputÃ¡lt fÃ¡jl mentve: /content/cpri_outputs/usa_daily_imputed.csv\n",
            "\n",
            "ðŸ”§ FeldolgozÃ¡s: nemet_daily_merged.csv\n",
            "âœ… ImputÃ¡lt fÃ¡jl mentve: /content/cpri_outputs/nemet_daily_imputed.csv\n",
            "\n",
            "ðŸ”§ FeldolgozÃ¡s: tajvan_daily_merged.csv\n",
            "âœ… ImputÃ¡lt fÃ¡jl mentve: /content/cpri_outputs/tajvan_daily_imputed.csv\n",
            "\n",
            "ðŸ”§ FeldolgozÃ¡s: dania_daily_merged.csv\n",
            "âœ… ImputÃ¡lt fÃ¡jl mentve: /content/cpri_outputs/dania_daily_imputed.csv\n",
            "\n",
            "ðŸ”§ FeldolgozÃ¡s: uk_daily_merged.csv\n",
            "âœ… ImputÃ¡lt fÃ¡jl mentve: /content/cpri_outputs/uk_daily_imputed.csv\n",
            "\n",
            "ðŸ KÃ‰SZ â€” nÃ©zd meg a cpri_outputs mappÃ¡t, ott lesznek az Ãºj *_daily_imputed.csv fÃ¡jlok.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# FeltÃ©telezve, hogy df a napi adatokat tartalmazza\n",
        "df['year'] = pd.to_datetime(df['date']).dt.year\n",
        "\n",
        "# Csak numerikus oszlopokat hagyunk meg\n",
        "num_cols = df.select_dtypes(include=[np.number]).columns.tolist() + ['year']\n",
        "df = df[num_cols]\n",
        "\n",
        "# Ã‰ves aggregÃ¡lÃ¡s\n",
        "df_yearly = df.groupby('year').mean().reset_index()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "rsDx_1raH_6S",
        "outputId": "09c7a554-6721-4f00-89dd-be528278b39b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Grouper for 'year' not 1-dimensional",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1659708971.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Ã‰ves aggregÃ¡lÃ¡s\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mdf_yearly\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'year'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mgroupby\u001b[0;34m(self, by, axis, level, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[1;32m   9181\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You have to supply one of 'by' and 'level'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 9183\u001b[0;31m         return DataFrameGroupBy(\n\u001b[0m\u001b[1;32m   9184\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9185\u001b[0m             \u001b[0mkeys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[1;32m   1327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgrouper\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m             grouper, exclusions, obj = get_grouper(\n\u001b[0m\u001b[1;32m   1330\u001b[0m                 \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m                 \u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/groupby/grouper.py\u001b[0m in \u001b[0;36mget_grouper\u001b[0;34m(obj, key, axis, level, sort, observed, validate, dropna)\u001b[0m\n\u001b[1;32m   1036\u001b[0m                     \u001b[0;31m# non-unique columns; raise here to get the name in the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m                     \u001b[0;31m# exception message\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1038\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Grouper for '{name}' not 1-dimensional\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1039\u001b[0m                 \u001b[0mexclusions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_level_reference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Grouper for 'year' not 1-dimensional"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- ðŸ“¦ KÃ¶nyvtÃ¡rak ---\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import glob, os\n",
        "\n",
        "# --- ðŸ“ BeÃ¡llÃ­tÃ¡sok ---\n",
        "input_dir = \"/content/cpri_outputs\"\n",
        "os.makedirs(input_dir, exist_ok=True)\n",
        "\n",
        "# --- ðŸŒ RÃ©giÃ³-sÃºlyok ---\n",
        "region_weights = {\n",
        "    \"dania\": 0.8, \"denmark\": 0.8,\n",
        "    \"nemet\": 0.6, \"germany\": 0.6,\n",
        "    \"uk\": 0.9, \"netherlands\": 0.9, \"holland\": 0.9,\n",
        "    \"usa\": 1.0, \"united states\": 1.0,\n",
        "    \"tajvan\": 1.0, \"taiwan\": 1.0\n",
        "}\n",
        "\n",
        "# --- âš™ï¸ Hazard vÃ¡ltozÃ³k ---\n",
        "hazard_vars = {\n",
        "    \"flood\": \"tp_sum_mean\",           # csapadÃ©k\n",
        "    \"heatwave\": \"t2m_mean_mean\",      # hÅ‘mÃ©rsÃ©klet\n",
        "    \"windstorm\": \"u10_max_mean\",      # szÃ©lsebessÃ©g\n",
        "    \"coldwave\": \"sd_mean_mean\",       # hÃ³/fagy\n",
        "    \"solaranomaly\": \"ssrd_mean_mean\"  # napfÃ©ny-anomÃ¡lia\n",
        "}\n",
        "\n",
        "# --- ðŸ§® Ã‰ves aggregÃ¡lÃ¡s minden daily fÃ¡jlra ---\n",
        "files = glob.glob(f\"{input_dir}/*_daily_imputed.csv\") or glob.glob(f\"{input_dir}/*_daily_merged.csv\")\n",
        "if not files:\n",
        "    raise FileNotFoundError(\"âš ï¸ Nincs napi CSV fÃ¡jl a megadott mappÃ¡ban!\")\n",
        "\n",
        "hazard_dfs = []\n",
        "\n",
        "for file in files:\n",
        "    region = os.path.basename(file).split(\"_\")[0].lower()\n",
        "    print(f\"\\nðŸ“‚ FeldolgozÃ¡s: {region}\")\n",
        "\n",
        "    df = pd.read_csv(file)\n",
        "\n",
        "    # dÃ¡tum Ã©s Ã©v kinyerÃ©se\n",
        "    df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
        "    df = df.dropna(subset=['date'])\n",
        "    df['year'] = df['date'].dt.year\n",
        "\n",
        "    # numerikus oszlopok\n",
        "    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    if 'year' not in num_cols:\n",
        "        num_cols.append('year')\n",
        "    df = df[num_cols]\n",
        "\n",
        "    # Ã©ves Ã¡tlagolÃ¡s\n",
        "    df_yearly = df.groupby('year').mean(numeric_only=True).reset_index()\n",
        "\n",
        "    # hazard-specifikus szÃ¡mÃ­tÃ¡s\n",
        "    for hz, col in hazard_vars.items():\n",
        "        if col not in df_yearly.columns:\n",
        "            continue\n",
        "\n",
        "        # nyers Ã©rtÃ©kek\n",
        "        H_raw = df_yearly[col].mean(skipna=True)\n",
        "        threshold = df_yearly[col].quantile(0.95)\n",
        "        P_raw = (df_yearly[col] > threshold).sum() / len(df_yearly)\n",
        "\n",
        "        # normalizÃ¡lÃ¡s\n",
        "        df_yearly[f\"{hz}_H_norm\"] = (df_yearly[col] - df_yearly[col].min()) / (df_yearly[col].max() - df_yearly[col].min() + 1e-9)\n",
        "        df_yearly[f\"{hz}_P_norm\"] = np.clip(P_raw, 0, 1)\n",
        "\n",
        "        # regionÃ¡lis sÃºlyozÃ¡s\n",
        "        weight = region_weights.get(region, 1.0)\n",
        "        df_yearly['E'] = (df_yearly[f\"{hz}_H_norm\"] + df_yearly[f\"{hz}_P_norm\"]) / 2 * weight\n",
        "        df_yearly['hazard'] = hz\n",
        "        df_yearly['region'] = region\n",
        "\n",
        "        hazard_dfs.append(df_yearly[['hazard', 'region', 'year', f\"{hz}_H_norm\", f\"{hz}_P_norm\", 'E']])\n",
        "\n",
        "# --- ðŸ“Š Ã–sszefÅ±zÃ©s ---\n",
        "hazard_all = pd.concat(hazard_dfs, ignore_index=True)\n",
        "hazard_all.to_csv(f\"{input_dir}/exposure_matrix.csv\", index=False)\n",
        "print(f\"\\nâœ… Exposure matrix mentve: {len(hazard_all)} sor\")\n",
        "\n",
        "# --- ðŸ§± Assetek betÃ¶ltÃ©se ---\n",
        "assets_path = f\"{input_dir}/assets.xlsx\"\n",
        "if not os.path.exists(assets_path):\n",
        "    raise FileNotFoundError(f\"âš ï¸ HiÃ¡nyzik az asset fÃ¡jl: {assets_path}\")\n",
        "\n",
        "assets = pd.read_excel(assets_path)\n",
        "\n",
        "# --- ðŸ’€ SÃ©rÃ¼lÃ©kenysÃ©g (V) szÃ¡mÃ­tÃ¡sa ---\n",
        "def get_V0(row):\n",
        "    type_map = {\n",
        "        (\"wind\", \"offshore\"): 0.6, (\"wind\", \"onshore\"): 0.4,\n",
        "        (\"solar\", \"pv\"): 0.3, (\"hydro\", \"-\"): 0.5,\n",
        "        (\"biomass\", \"-\"): 0.4, (\"gas\", \"-\"): 0.7,\n",
        "        (\"coal\", \"-\"): 0.7, (\"nuclear\", \"-\"): 0.8,\n",
        "        (\"hydrogen\", \"-\"): 0.9, (\"storage\", \"battery\"): 0.7,\n",
        "        (\"grid\", \"transmission\"): 0.6,\n",
        "    }\n",
        "    return type_map.get((str(row['type']).lower(), str(row['subtype']).lower()), 0.5)\n",
        "\n",
        "assets[\"V0\"] = assets.apply(get_V0, axis=1)\n",
        "assets[\"age_factor\"] = 2025 - assets[\"commission_year\"]\n",
        "assets[\"maintenance\"] = 0.7\n",
        "assets[\"V\"] = np.clip(\n",
        "    assets[\"V0\"] * (1 + 0.3 * assets[\"age_factor\"] / 50) * (1 - 0.4 * assets[\"maintenance\"]),\n",
        "    0, 1\n",
        ")\n",
        "assets.to_csv(f\"{input_dir}/vulnerability_matrix.csv\", index=False)\n",
        "print(f\"âœ… Vulnerability matrix mentve ({len(assets)} sor)\")\n",
        "\n",
        "# --- âš¡ r Ã©rtÃ©kek szÃ¡mÃ­tÃ¡sa ---\n",
        "results = []\n",
        "for _, a in assets.iterrows():\n",
        "    reg = a[\"country\"].lower()\n",
        "    subset = hazard_all[hazard_all[\"region\"].str.contains(reg[:3], na=False)]\n",
        "    for _, s in subset.iterrows():\n",
        "        H_cols = [c for c in s.index if \"H_norm\" in c]\n",
        "        P_cols = [c for c in s.index if \"P_norm\" in c]\n",
        "        H = s[H_cols[0]] if H_cols else np.nan\n",
        "        P = s[P_cols[0]] if P_cols else np.nan\n",
        "        E = s[\"E\"]\n",
        "        V = a[\"V\"]\n",
        "        r = H * P * E * V if not pd.isna(H) and not pd.isna(P) else 0\n",
        "        results.append({\n",
        "            \"asset_id\": a[\"asset_id\"],\n",
        "            \"hazard\": s[\"hazard\"],\n",
        "            \"region\": s[\"region\"],\n",
        "            \"year\": s[\"year\"],\n",
        "            \"H_norm\": H,\n",
        "            \"P_norm\": P,\n",
        "            \"E\": E,\n",
        "            \"V\": V,\n",
        "            \"r\": r\n",
        "        })\n",
        "\n",
        "df_r = pd.DataFrame(results)\n",
        "df_r.to_csv(f\"{input_dir}/r_values_enhanced.csv\", index=False)\n",
        "print(f\"\\nâœ… Mentve: {input_dir}/r_values_enhanced.csv ({len(df_r)} sor)\")\n",
        "\n",
        "# --- ðŸ“ˆ Diagnosztika ---\n",
        "print(\"\\n--- Diagnosztika ---\")\n",
        "for hz in df_r['hazard'].unique():\n",
        "    sub = df_r[df_r['hazard'] == hz]\n",
        "    total = len(sub)\n",
        "    zeros = (sub['r'] == 0).sum()\n",
        "    print(f\"{hz:15} â†’ {zeros}/{total} = {zeros/total:.1%} nullÃ¡s Ã©rtÃ©k\")\n",
        "\n",
        "print(\"\\nðŸ KÃ©sz, minden lÃ©pÃ©s sikeresen lefutott!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQ4JJGpW6YAJ",
        "outputId": "f197be19-5033-4a54-8d47-26e2ad1769ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“‚ FeldolgozÃ¡s: dania\n",
            "\n",
            "ðŸ“‚ FeldolgozÃ¡s: usa\n",
            "\n",
            "ðŸ“‚ FeldolgozÃ¡s: uk\n",
            "\n",
            "ðŸ“‚ FeldolgozÃ¡s: nemet\n",
            "\n",
            "ðŸ“‚ FeldolgozÃ¡s: tajvan\n",
            "\n",
            "âœ… Exposure matrix mentve: 528 sor\n",
            "âœ… Vulnerability matrix mentve (26 sor)\n",
            "\n",
            "âœ… Mentve: /content/cpri_outputs/r_values_enhanced.csv (1008 sor)\n",
            "\n",
            "--- Diagnosztika ---\n",
            "flood           â†’ 9/216 = 4.2% nullÃ¡s Ã©rtÃ©k\n",
            "heatwave        â†’ 216/216 = 100.0% nullÃ¡s Ã©rtÃ©k\n",
            "windstorm       â†’ 216/216 = 100.0% nullÃ¡s Ã©rtÃ©k\n",
            "coldwave        â†’ 144/144 = 100.0% nullÃ¡s Ã©rtÃ©k\n",
            "solaranomaly    â†’ 216/216 = 100.0% nullÃ¡s Ã©rtÃ©k\n",
            "\n",
            "ðŸ KÃ©sz, minden lÃ©pÃ©s sikeresen lefutott!\n"
          ]
        }
      ]
    }
  ]
}