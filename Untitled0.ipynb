{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNo7fmM1Wogr8846hhXwRAq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/esb-index/Barka-AV/blob/main/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/era5_processed_light && mv /content/*_daily_light.csv /content/era5_processed_light/\n"
      ],
      "metadata": {
        "id": "VwHKuU6s7xZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1Ô∏è‚É£ Importok ---\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from scipy.stats.mstats import winsorize\n",
        "\n",
        "# --- 2Ô∏è‚É£ Be√°ll√≠t√°sok ---\n",
        "INPUT_PATH = \"/content/\"\n",
        "OUTPUT_PATH = \"/content/cpri_outputs/\"\n",
        "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
        "\n",
        "# --- 3Ô∏è‚É£ Orsz√°gok beolvas√°sa ---\n",
        "files = [f for f in os.listdir(INPUT_PATH) if f.endswith(\"_daily_light.csv\")]\n",
        "print(\"Tal√°lt napi f√°jlok:\", files)\n",
        "\n",
        "# --- 4Ô∏è‚É£ Helper: winsoriz√°l√°s + normaliz√°l√°s ---\n",
        "def winsorize_series(s):\n",
        "    return winsorize(s, limits=[0.01, 0.01])\n",
        "\n",
        "def normalize_series(s):\n",
        "    return (s - s.min()) / (s.max() - s.min()) if s.max() != s.min() else s*0\n",
        "\n",
        "# --- 5Ô∏è‚É£ Feldolgoz√°s ---\n",
        "yearly_records = {\"heatwave\": [], \"flood\": [], \"windstorm\": []}\n",
        "\n",
        "for f in tqdm(files):\n",
        "    df = pd.read_csv(os.path.join(INPUT_PATH, f))\n",
        "    country = f.split(\"_\")[0].lower()\n",
        "\n",
        "    # biztos√≠tjuk, hogy legyen 'date' √©s 't2m', 'tp', 'u10', 'v10' oszlop\n",
        "    if 'date' not in df.columns:\n",
        "        df.rename(columns={c: 'date' for c in df.columns if 'time' in c.lower()}, inplace=True)\n",
        "    df['date'] = pd.to_datetime(df['date'])\n",
        "    df['year'] = df['date'].dt.year\n",
        "\n",
        "    #  --- HEATWAVE: 2m_temp > 90. percentil ---\n",
        "    if 't2m' in df.columns:\n",
        "        t2m = df['t2m']\n",
        "        thresh = np.nanpercentile(t2m, 90)\n",
        "        df['heatwave_flag'] = (t2m > thresh).astype(int)\n",
        "        hw_yearly = df.groupby('year').agg(\n",
        "            H_raw=('t2m', 'mean'),\n",
        "            P_raw=('heatwave_flag', 'sum')\n",
        "        ).reset_index()\n",
        "        hw_yearly['region'] = country\n",
        "        yearly_records['heatwave'].append(hw_yearly)\n",
        "\n",
        "    # --- FLOOD: total_precipitation > 95. percentil ---\n",
        "    if 'tp' in df.columns:\n",
        "        tp = df['tp']\n",
        "        thresh = np.nanpercentile(tp, 95)\n",
        "        df['flood_flag'] = (tp > thresh).astype(int)\n",
        "        fl_yearly = df.groupby('year').agg(\n",
        "            H_raw=('tp', 'sum'),\n",
        "            P_raw=('flood_flag', 'sum')\n",
        "        ).reset_index()\n",
        "        fl_yearly['region'] = country\n",
        "        yearly_records['flood'].append(fl_yearly)\n",
        "\n",
        "    # --- WINDSTORM: 10m sz√©lsebess√©g (sqrt(u10¬≤+v10¬≤)) > 95. percentil ---\n",
        "    if {'u10','v10'}.issubset(df.columns):\n",
        "        df['wind_speed'] = np.sqrt(df['u10']**2 + df['v10']**2)\n",
        "        thresh = np.nanpercentile(df['wind_speed'], 95)\n",
        "        df['storm_flag'] = (df['wind_speed'] > thresh).astype(int)\n",
        "        ws_yearly = df.groupby('year').agg(\n",
        "            H_raw=('wind_speed', 'mean'),\n",
        "            P_raw=('storm_flag', 'sum')\n",
        "        ).reset_index()\n",
        "        ws_yearly['region'] = country\n",
        "        yearly_records['windstorm'].append(ws_yearly)\n",
        "\n",
        "# --- 6Ô∏è‚É£ √ñsszes√≠t√©s, winsoriz√°l√°s √©s normaliz√°l√°s ---\n",
        "for hazard, dfs in yearly_records.items():\n",
        "    if len(dfs) == 0:\n",
        "        continue\n",
        "    df_all = pd.concat(dfs, ignore_index=True)\n",
        "    df_all['H_win'] = winsorize_series(df_all['H_raw'])\n",
        "    df_all['P_win'] = winsorize_series(df_all['P_raw'])\n",
        "    df_all['H_norm'] = normalize_series(df_all['H_win'])\n",
        "    df_all['P_norm'] = normalize_series(df_all['P_win'])\n",
        "    df_all.to_csv(os.path.join(OUTPUT_PATH, f\"hazard_yearly_{hazard}.csv\"), index=False)\n",
        "    print(f\"‚úÖ Mentve: hazard_yearly_{hazard}.csv ({df_all.shape[0]} sor)\")\n",
        "\n",
        "print(\"\\nüéØ K√©sz: √∂sszes hazard √∫jragener√°lva a 'cpri_outputs' mapp√°ba!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "elbTxvYoQ6NA",
        "outputId": "85de1b8c-7f5b-4bc0-b97e-2b8c5747a724"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tal√°lt napi f√°jlok: ['uk_daily_light.csv', 'nemet_daily_light.csv', 'usa_daily_light.csv', 'tajvan_daily_light.csv', 'dania_daily_light.csv']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:00<00:00, 18.87it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üéØ K√©sz: √∂sszes hazard √∫jragener√°lva a 'cpri_outputs' mapp√°ba!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(\"üìÇ Aktu√°lis tartalom a /content-ben:\")\n",
        "for f in os.listdir(\"/content\"):\n",
        "    print(\" -\", f)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DXn935W9Su1-",
        "outputId": "831c4ce5-2ac6-4707-923a-8c6f4e749a7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÇ Aktu√°lis tartalom a /content-ben:\n",
            " - .config\n",
            " - uk_daily_light.csv\n",
            " - nemet_daily_light.csv\n",
            " - usa_daily_light.csv\n",
            " - cpri_outputs\n",
            " - tajvan_daily_light.csv\n",
            " - dania_daily_light.csv\n",
            " - sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"/content/dania_daily_light.csv\")\n",
        "print(\"Sorok sz√°ma:\", len(df))\n",
        "print(\"Oszlopok:\", list(df.columns))\n",
        "print(df.head(3))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oz1UX0crS_0i",
        "outputId": "0ae0bf44-75fd-4387-dfa4-2ba4df73d05a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sorok sz√°ma: 17532\n",
            "Oszlopok: ['date', 'tp_sum_mean', 'ssrd_mean_mean', 'u10_max_mean', 'v10_max_mean', 't2m_mean_mean', 'msl_mean_mean', 'sst_mean_mean', 'sd_mean_mean']\n",
            "         date  tp_sum_mean  ssrd_mean_mean  u10_max_mean  v10_max_mean  \\\n",
            "0  2000-01-01     0.000268       266652.60           NaN           NaN   \n",
            "1  2000-01-01          NaN             NaN       4.70326       0.92689   \n",
            "2  2000-01-02     0.000019       190477.27           NaN           NaN   \n",
            "\n",
            "   t2m_mean_mean  msl_mean_mean  sst_mean_mean  sd_mean_mean  \n",
            "0            NaN            NaN            NaN           NaN  \n",
            "1      278.23892      101576.28         277.97       0.00017  \n",
            "2            NaN            NaN            NaN           NaN  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import glob, os\n",
        "\n",
        "# --- Seg√©df√ºggv√©ny a winsoriz√°l√°shoz ---\n",
        "def winsorize_series(s, low=1, high=99):\n",
        "    lowv, highv = np.nanpercentile(s, [low, high])\n",
        "    return np.clip(s, lowv, highv)\n",
        "\n",
        "# --- Input f√°jlok keres√©se ---\n",
        "files = glob.glob(\"/content/*_daily_light.csv\")\n",
        "print(f\"üîç Tal√°lt f√°jlok: {len(files)}\")\n",
        "\n",
        "results = []\n",
        "\n",
        "for f in files:\n",
        "    region = os.path.basename(f).replace(\"_daily_light.csv\", \"\")\n",
        "    print(f\"\\nüåç Feldolgoz√°s: {region}\")\n",
        "\n",
        "    df = pd.read_csv(f, parse_dates=['date'])\n",
        "    df['year'] = pd.to_datetime(df['date']).dt.year\n",
        "\n",
        "    # --- Az oszlopnevek automatikus felismer√©se ---\n",
        "    tp_col = [c for c in df.columns if 'tp' in c][0]\n",
        "    t2m_col = [c for c in df.columns if 't2m' in c][0]\n",
        "    u_col = [c for c in df.columns if 'u10' in c][0]\n",
        "    v_col = [c for c in df.columns if 'v10' in c][0]\n",
        "\n",
        "    # --- WINDSTORM ---\n",
        "    df['wind_speed'] = np.sqrt(df[u_col]**2 + df[v_col]**2)\n",
        "    df_ws = df.groupby('year')['wind_speed'].agg(['mean', 'max', 'count']).reset_index()\n",
        "    df_ws['H_raw'] = df_ws['max']\n",
        "    df_ws['P_raw'] = df_ws['mean']\n",
        "    df_ws['hazard'] = 'windstorm'\n",
        "    results.append(df_ws.assign(region=region))\n",
        "\n",
        "    # --- HEATWAVE ---\n",
        "    df['t2m_C'] = df[t2m_col] - 273.15\n",
        "    threshold = np.nanpercentile(df['t2m_C'], 95)\n",
        "    df['is_hot'] = df['t2m_C'] > threshold\n",
        "    df_hw = df.groupby('year')['is_hot'].sum().reset_index()\n",
        "    df_hw['H_raw'] = threshold\n",
        "    df_hw['P_raw'] = df_hw['is_hot']\n",
        "    df_hw['hazard'] = 'heatwave'\n",
        "    results.append(df_hw.assign(region=region))\n",
        "\n",
        "    # --- FLOOD (precipitation) ---\n",
        "    df_fl = df.groupby('year')[tp_col].sum().reset_index()\n",
        "    df_fl['H_raw'] = df_fl[tp_col]\n",
        "    df_fl['P_raw'] = (df_fl[tp_col] > np.nanpercentile(df_fl[tp_col], 90)).astype(int)\n",
        "    df_fl['hazard'] = 'flood'\n",
        "    results.append(df_fl.assign(region=region))\n",
        "\n",
        "# --- Egyes√≠t√©s √©s normaliz√°l√°s ---\n",
        "all_df = pd.concat(results, ignore_index=True)\n",
        "all_df['H_win'] = all_df.groupby('hazard')['H_raw'].transform(winsorize_series)\n",
        "all_df['P_win'] = all_df.groupby('hazard')['P_raw'].transform(winsorize_series)\n",
        "\n",
        "# Normaliz√°l√°s hazardonk√©nt (min‚Äìmax)\n",
        "def normalize(g):\n",
        "    g['H_norm'] = (g['H_win'] - g['H_win'].min()) / (g['H_win'].max() - g['H_win'].min())\n",
        "    g['P_norm'] = (g['P_win'] - g['P_win'].min()) / (g['P_win'].max() - g['P_win'].min())\n",
        "    return g\n",
        "\n",
        "all_df = all_df.groupby('hazard', group_keys=False).apply(normalize)\n",
        "\n",
        "# --- Kimenetek k√ºl√∂n f√°jlba ---\n",
        "os.makedirs(\"/content/cpri_outputs\", exist_ok=True)\n",
        "for hazard in ['flood', 'windstorm', 'heatwave']:\n",
        "    subset = all_df[all_df['hazard'] == hazard]\n",
        "    out_path = f\"/content/cpri_outputs/hazard_yearly_{hazard}.csv\"\n",
        "    subset.to_csv(out_path, index=False)\n",
        "    print(f\"‚úÖ Mentve: {out_path} ({len(subset)} sor)\")\n",
        "\n",
        "print(\"\\nüéØ Minden hazard f√°jl sikeresen elk√©sz√ºlt!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "La7WGvVBT4_A",
        "outputId": "ed2c5aae-ecf0-4a9b-986d-167bdd25ef8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Tal√°lt f√°jlok: 5\n",
            "\n",
            "üåç Feldolgoz√°s: uk\n",
            "\n",
            "üåç Feldolgoz√°s: nemet\n",
            "\n",
            "üåç Feldolgoz√°s: usa\n",
            "\n",
            "üåç Feldolgoz√°s: tajvan\n",
            "\n",
            "üåç Feldolgoz√°s: dania\n",
            "‚úÖ Mentve: /content/cpri_outputs/hazard_yearly_flood.csv (120 sor)\n",
            "‚úÖ Mentve: /content/cpri_outputs/hazard_yearly_windstorm.csv (120 sor)\n",
            "‚úÖ Mentve: /content/cpri_outputs/hazard_yearly_heatwave.csv (120 sor)\n",
            "\n",
            "üéØ Minden hazard f√°jl sikeresen elk√©sz√ºlt!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2264031953.py:65: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  all_df = all_df.groupby('hazard', group_keys=False).apply(normalize)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import glob, os\n",
        "\n",
        "# --- Be√°ll√≠t√°sok ---\n",
        "ASSETS_XLSX = \"/content/assets.xlsx\"\n",
        "HAZARD_PATH = \"/content/cpri_outputs/\"\n",
        "OUTPUT_PATH = \"/content/cpri_outputs/\"\n",
        "\n",
        "# --- 1Ô∏è‚É£ Assetek beolvas√°sa ---\n",
        "assets = pd.read_excel(ASSETS_XLSX)\n",
        "assets.rename(columns={\n",
        "    'latitude': 'Latitude',\n",
        "    'longitude': 'Longitude'\n",
        "}, inplace=True)\n",
        "\n",
        "assets['Latitude'] = pd.to_numeric(assets['Latitude'], errors='coerce')\n",
        "assets['Longitude'] = pd.to_numeric(assets['Longitude'], errors='coerce')\n",
        "\n",
        "print(f\"üìä Beolvasott assetek: {assets.shape[0]} db\")\n",
        "\n",
        "# --- 2Ô∏è‚É£ Eredm√©nylista ---\n",
        "results = []\n",
        "\n",
        "# --- 3Ô∏è‚É£ Hazard f√°jlok bej√°r√°sa ---\n",
        "hazard_files = glob.glob(os.path.join(HAZARD_PATH, \"hazard_yearly_*.csv\"))\n",
        "\n",
        "for hf in hazard_files:\n",
        "    hazard_name = os.path.basename(hf).replace(\"hazard_yearly_\", \"\").replace(\".csv\", \"\")\n",
        "    df = pd.read_csv(hf)\n",
        "\n",
        "    print(f\"\\nüåç Feldolgoz√°s: {hazard_name} ‚Äî {len(df)} sor\")\n",
        "\n",
        "    # minden r√©gi√≥hoz kapcsoljuk az ottani asseteket\n",
        "\n",
        "# Orsz√°gnevek lek√©pez√©se a hazard-f√°jlok r√∂vid neveire\n",
        "region_map = {\n",
        "    \"denmark\": \"dania\",\n",
        "    \"uk\": \"uk\",\n",
        "    \"united kingdom\": \"uk\",\n",
        "    \"germany\": \"nemet\",\n",
        "    \"netherlands\": \"uk\",  # holland adatok az UK f√°jlban voltak\n",
        "    \"taiwan\": \"tajvan\",\n",
        "    \"usa\": \"usa\",\n",
        "    \"united states\": \"usa\",\n",
        "    \"us\": \"usa\"\n",
        "}\n",
        "\n",
        "for _, a in assets.iterrows():\n",
        "    country = a['country'].strip().lower()\n",
        "    region = region_map.get(country, country)\n",
        "\n",
        "    subset = df[df['region'].str.lower() == region]\n",
        "    if subset.empty:\n",
        "            continue\n",
        "\n",
        "    for _, s in subset.iterrows():\n",
        "            year = s['year']\n",
        "            H = s['H_norm']\n",
        "            P = s['P_norm']\n",
        "            E = 1.0  # ideiglenes expoz√≠ci√≥\n",
        "            V = 0.5  # baseline s√©r√ºl√©kenys√©g\n",
        "            r = H * P * E * V\n",
        "            results.append({\n",
        "                'asset_id': a['asset_id'],\n",
        "                'hazard': hazard_name,\n",
        "                'year': year,\n",
        "                'H_norm': H,\n",
        "                'P_norm': P,\n",
        "                'E': E,\n",
        "                'V': V,\n",
        "                'r': r\n",
        "            })\n",
        "\n",
        "# --- 4Ô∏è‚É£ Eredm√©ny ment√©se ---\n",
        "df_r = pd.DataFrame(results)\n",
        "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
        "out_path = os.path.join(OUTPUT_PATH, \"r_values.csv\")\n",
        "df_r.to_csv(out_path, index=False)\n",
        "\n",
        "print(f\"\\n‚úÖ Mentve: {out_path}\")\n",
        "print(f\"üìà √ñsszes sor: {df_r.shape[0]}\")\n",
        "print(df_r.head(10))\n"
      ],
      "metadata": {
        "id": "msdIsGY8VjRg",
        "outputId": "e5881e23-d3ec-4ed7-e748-6304746d88c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä Beolvasott assetek: 20 db\n",
            "\n",
            "üåç Feldolgoz√°s: windstorm ‚Äî 120 sor\n",
            "\n",
            "üåç Feldolgoz√°s: heatwave ‚Äî 120 sor\n",
            "\n",
            "üåç Feldolgoz√°s: flood ‚Äî 120 sor\n",
            "\n",
            "‚úÖ Mentve: /content/cpri_outputs/r_values.csv\n",
            "üìà √ñsszes sor: 480\n",
            "  asset_id hazard  year    H_norm  P_norm    E    V    r\n",
            "0   DK-001  flood  2000  0.340112     0.0  1.0  0.5  0.0\n",
            "1   DK-001  flood  2001  0.255916     0.0  1.0  0.5  0.0\n",
            "2   DK-001  flood  2002  0.199476     0.0  1.0  0.5  0.0\n",
            "3   DK-001  flood  2003  0.118458     0.0  1.0  0.5  0.0\n",
            "4   DK-001  flood  2004  0.255484     0.0  1.0  0.5  0.0\n",
            "5   DK-001  flood  2005  0.130603     0.0  1.0  0.5  0.0\n",
            "6   DK-001  flood  2006  0.321203     0.0  1.0  0.5  0.0\n",
            "7   DK-001  flood  2007  0.281886     0.0  1.0  0.5  0.0\n",
            "8   DK-001  flood  2008  0.239728     0.0  1.0  0.5  0.0\n",
            "9   DK-001  flood  2009  0.200225     0.0  1.0  0.5  0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# --- Beolvas√°s ---\n",
        "df = pd.read_csv(\"/content/cpri_outputs/r_values.csv\")\n",
        "\n",
        "# --- Biztos ami biztos ---\n",
        "df['r'] = pd.to_numeric(df['r'], errors='coerce')\n",
        "\n",
        "# --- 1Ô∏è‚É£ √âves CPRI per asset-hazard ---\n",
        "agg = df.groupby(['asset_id', 'hazard', 'year'], as_index=False)['r'].mean()\n",
        "\n",
        "# --- 2Ô∏è‚É£ √ñsszes√≠tett CPRI per asset (hazardok √°tlag√°val) ---\n",
        "cpri = agg.groupby(['asset_id', 'year'], as_index=False)['r'].mean()\n",
        "cpri.rename(columns={'r': 'CPRI_index'}, inplace=True)\n",
        "\n",
        "# --- 3Ô∏è‚É£ Normaliz√°l√°s (0‚Äì1 sk√°l√°n minden √©vre k√ºl√∂n) ---\n",
        "cpri['CPRI_norm'] = cpri.groupby('year')['CPRI_index'].transform(\n",
        "    lambda x: (x - x.min()) / (x.max() - x.min())\n",
        ")\n",
        "\n",
        "# --- 4Ô∏è‚É£ Kimenet ment√©se ---\n",
        "out_path = \"/content/cpri_outputs/cpri_index.csv\"\n",
        "cpri.to_csv(out_path, index=False)\n",
        "\n",
        "print(f\"‚úÖ Mentve: {out_path}\")\n",
        "print(f\"üìà Sorok sz√°ma: {cpri.shape[0]}\")\n",
        "print(\"\\nüìä Minta:\")\n",
        "print(cpri.head(10))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IEL-Xne3ZsHY",
        "outputId": "5a62a479-5311-4334-9033-c027e5af844a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Mentve: /content/cpri_outputs/cpri_index.csv\n",
            "üìà Sorok sz√°ma: 480\n",
            "\n",
            "üìä Minta:\n",
            "  asset_id  year  CPRI_index  CPRI_norm\n",
            "0   DE-001  2000         0.0        0.0\n",
            "1   DE-001  2001         0.0        NaN\n",
            "2   DE-001  2002         0.0        NaN\n",
            "3   DE-001  2003         0.0        NaN\n",
            "4   DE-001  2004         0.0        NaN\n",
            "5   DE-001  2005         0.0        NaN\n",
            "6   DE-001  2006         0.0        0.0\n",
            "7   DE-001  2007         0.0        NaN\n",
            "8   DE-001  2008         0.0        NaN\n",
            "9   DE-001  2009         0.0        0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# N√©met hazard f√°jlok megn√©z√©se\n",
        "for hazard in ['flood', 'windstorm', 'heatwave']:\n",
        "    df = pd.read_csv(f\"/content/cpri_outputs/hazard_yearly_{hazard}.csv\")\n",
        "    print(f\"\\n=== {hazard.upper()} ‚Äî p√©ldasorok N√©metorsz√°gra ===\")\n",
        "    print(df[df['region'] == 'nemet'].head(10))\n"
      ],
      "metadata": {
        "id": "2rA4a26Ekurk",
        "outputId": "0a768401-b0f3-45c1-a7fb-56db9287bcfe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== FLOOD ‚Äî p√©ldasorok N√©metorsz√°gra ===\n",
            "    year  mean  max  count     H_raw  P_raw hazard region  is_hot  \\\n",
            "24  2000   NaN  NaN    NaN  0.036424    0.0  flood  nemet     NaN   \n",
            "25  2001   NaN  NaN    NaN  0.039963    0.0  flood  nemet     NaN   \n",
            "26  2002   NaN  NaN    NaN  0.036833    0.0  flood  nemet     NaN   \n",
            "27  2003   NaN  NaN    NaN  0.023013    0.0  flood  nemet     NaN   \n",
            "28  2004   NaN  NaN    NaN  0.039967    0.0  flood  nemet     NaN   \n",
            "29  2005   NaN  NaN    NaN  0.022228    0.0  flood  nemet     NaN   \n",
            "30  2006   NaN  NaN    NaN  0.034454    0.0  flood  nemet     NaN   \n",
            "31  2007   NaN  NaN    NaN  0.036744    0.0  flood  nemet     NaN   \n",
            "32  2008   NaN  NaN    NaN  0.033625    0.0  flood  nemet     NaN   \n",
            "33  2009   NaN  NaN    NaN  0.040384    0.0  flood  nemet     NaN   \n",
            "\n",
            "    tp_sum_mean     H_win  P_win    H_norm  P_norm  \n",
            "24     0.036424  0.036424    0.0  0.226529     0.0  \n",
            "25     0.039963  0.039963    0.0  0.283613     0.0  \n",
            "26     0.036833  0.036833    0.0  0.233134     0.0  \n",
            "27     0.023013  0.023013    0.0  0.010251     0.0  \n",
            "28     0.039967  0.039967    0.0  0.283674     0.0  \n",
            "29     0.022228  0.022378    0.0  0.000000     0.0  \n",
            "30     0.034454  0.034454    0.0  0.194771     0.0  \n",
            "31     0.036744  0.036744    0.0  0.231699     0.0  \n",
            "32     0.033625  0.033625    0.0  0.181397     0.0  \n",
            "33     0.040384  0.040384    0.0  0.290397     0.0  \n",
            "\n",
            "=== WINDSTORM ‚Äî p√©ldasorok N√©metorsz√°gra ===\n",
            "    year      mean        max  count      H_raw     P_raw     hazard region  \\\n",
            "24  2000  8.333054  17.117221  366.0  17.117221  8.333054  windstorm  nemet   \n",
            "25  2001  7.666047  15.564889  365.0  15.564889  7.666047  windstorm  nemet   \n",
            "26  2002  7.829902  18.528729  365.0  18.528729  7.829902  windstorm  nemet   \n",
            "27  2003  7.214494  16.526174  365.0  16.526174  7.214494  windstorm  nemet   \n",
            "28  2004  7.707999  16.530238  366.0  16.530238  7.707999  windstorm  nemet   \n",
            "29  2005  8.042427  19.035973  365.0  19.035973  8.042427  windstorm  nemet   \n",
            "30  2006  7.622988  16.497421  365.0  16.497421  7.622988  windstorm  nemet   \n",
            "31  2007  8.311511  17.723574  365.0  17.723574  8.311511  windstorm  nemet   \n",
            "32  2008  8.236498  17.534645  366.0  17.534645  8.236498  windstorm  nemet   \n",
            "33  2009  7.731609  19.357107  365.0  19.357107  7.731609  windstorm  nemet   \n",
            "\n",
            "    is_hot  tp_sum_mean      H_win     P_win    H_norm    P_norm  \n",
            "24     NaN          NaN  17.117221  8.298009  0.816199  1.000000  \n",
            "25     NaN          NaN  15.564889  7.666047  0.662670  0.875361  \n",
            "26     NaN          NaN  18.528729  7.829902  0.955800  0.907677  \n",
            "27     NaN          NaN  16.526174  7.214494  0.757743  0.786303  \n",
            "28     NaN          NaN  16.530238  7.707999  0.758145  0.883635  \n",
            "29     NaN          NaN  18.975634  8.042427  1.000000  0.949593  \n",
            "30     NaN          NaN  16.497421  7.622988  0.754900  0.866868  \n",
            "31     NaN          NaN  17.723574  8.298009  0.876169  1.000000  \n",
            "32     NaN          NaN  17.534645  8.236498  0.857483  0.987869  \n",
            "33     NaN          NaN  18.975634  7.731609  1.000000  0.888291  \n",
            "\n",
            "=== HEATWAVE ‚Äî p√©ldasorok N√©metorsz√°gra ===\n",
            "    year  mean  max  count     H_raw  P_raw    hazard region  is_hot  \\\n",
            "24  2000   NaN  NaN    NaN  18.34994    4.0  heatwave  nemet     4.0   \n",
            "25  2001   NaN  NaN    NaN  18.34994   17.0  heatwave  nemet    17.0   \n",
            "26  2002   NaN  NaN    NaN  18.34994   38.0  heatwave  nemet    38.0   \n",
            "27  2003   NaN  NaN    NaN  18.34994   32.0  heatwave  nemet    32.0   \n",
            "28  2004   NaN  NaN    NaN  18.34994   19.0  heatwave  nemet    19.0   \n",
            "29  2005   NaN  NaN    NaN  18.34994    9.0  heatwave  nemet     9.0   \n",
            "30  2006   NaN  NaN    NaN  18.34994   37.0  heatwave  nemet    37.0   \n",
            "31  2007   NaN  NaN    NaN  18.34994    4.0  heatwave  nemet     4.0   \n",
            "32  2008   NaN  NaN    NaN  18.34994   11.0  heatwave  nemet    11.0   \n",
            "33  2009   NaN  NaN    NaN  18.34994   16.0  heatwave  nemet    16.0   \n",
            "\n",
            "    tp_sum_mean     H_win  P_win  H_norm    P_norm  \n",
            "24          NaN  18.34994    4.0     0.0  0.069178  \n",
            "25          NaN  18.34994   17.0     0.0  0.389217  \n",
            "26          NaN  18.34994   38.0     0.0  0.906204  \n",
            "27          NaN  18.34994   32.0     0.0  0.758493  \n",
            "28          NaN  18.34994   19.0     0.0  0.438454  \n",
            "29          NaN  18.34994    9.0     0.0  0.192270  \n",
            "30          NaN  18.34994   37.0     0.0  0.881585  \n",
            "31          NaN  18.34994    4.0     0.0  0.069178  \n",
            "32          NaN  18.34994   11.0     0.0  0.241507  \n",
            "33          NaN  18.34994   16.0     0.0  0.364599  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "\n",
        "# --- A CSV-k el√©r√©si √∫tjai ---\n",
        "paths = [\n",
        "    \"/content/cpri_outputs/hazard_yearly_flood.csv\",\n",
        "    \"/content/cpri_outputs/hazard_yearly_windstorm.csv\",\n",
        "    \"/content/cpri_outputs/hazard_yearly_heatwave.csv\",\n",
        "    \"/content/cpri_outputs/r_values.csv\",\n",
        "    \"/content/cpri_outputs/cpri_index.csv\"\n",
        "]\n",
        "\n",
        "combined = []\n",
        "\n",
        "# --- √ñsszef≈±z√©s, csak ha a f√°jl l√©tezik ---\n",
        "for p in paths:\n",
        "    if os.path.exists(p):\n",
        "        df = pd.read_csv(p)\n",
        "        df['source'] = os.path.basename(p)  # √∫j oszlop: forr√°s\n",
        "        combined.append(df)\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è Hi√°nyzik: {p}\")\n",
        "\n",
        "if combined:\n",
        "    df_all = pd.concat(combined, ignore_index=True)\n",
        "    out_path = \"/content/combined_cpri_data.csv\"\n",
        "    df_all.to_csv(out_path, index=False)\n",
        "    print(f\"‚úÖ √ñsszef≈±zve: {out_path}\")\n",
        "    print(f\"üìà √ñsszes sor: {len(df_all)}\")\n",
        "else:\n",
        "    print(\"‚ùå Nem tal√°ltam egyetlen forr√°st sem!\")\n"
      ],
      "metadata": {
        "id": "FoLu_FavGaq-",
        "outputId": "28cefea7-651a-4b64-ee43-76df024f8359",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ √ñsszef≈±zve: /content/combined_cpri_data.csv\n",
            "üìà √ñsszes sor: 1320\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"/content/combined_cpri_data.csv\")\n",
        "\n",
        "print(\"Sorok sz√°ma:\", len(df))\n",
        "print(\"Oszlopnevek:\", list(df.columns))\n",
        "print(\"\\nForr√°sf√°jlok:\", df['source'].unique())\n",
        "print(\"\\nEls≈ë 5 sor:\")\n",
        "print(df.head(5))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "loyLxaDqHxFy",
        "outputId": "bb923037-d4ac-45ba-c15a-5a7de06dd6a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sorok sz√°ma: 1320\n",
            "Oszlopnevek: ['year', 'mean', 'max', 'count', 'H_raw', 'P_raw', 'hazard', 'region', 'is_hot', 'tp_sum_mean', 'H_win', 'P_win', 'H_norm', 'P_norm', 'source', 'asset_id', 'E', 'V', 'r', 'CPRI_index', 'CPRI_norm']\n",
            "\n",
            "Forr√°sf√°jlok: ['hazard_yearly_flood.csv' 'hazard_yearly_windstorm.csv'\n",
            " 'hazard_yearly_heatwave.csv' 'r_values.csv' 'cpri_index.csv']\n",
            "\n",
            "Els≈ë 5 sor:\n",
            "   year  mean  max  count     H_raw  P_raw hazard region  is_hot  tp_sum_mean  \\\n",
            "0  2000   NaN  NaN    NaN  0.045559    1.0  flood     uk     NaN     0.045559   \n",
            "1  2001   NaN  NaN    NaN  0.038407    0.0  flood     uk     NaN     0.038407   \n",
            "2  2002   NaN  NaN    NaN  0.042355    0.0  flood     uk     NaN     0.042355   \n",
            "3  2003   NaN  NaN    NaN  0.033754    0.0  flood     uk     NaN     0.033754   \n",
            "4  2004   NaN  NaN    NaN  0.038914    0.0  flood     uk     NaN     0.038914   \n",
            "\n",
            "   ...  P_win    H_norm  P_norm                   source asset_id   E   V   r  \\\n",
            "0  ...    1.0  0.373858     1.0  hazard_yearly_flood.csv      NaN NaN NaN NaN   \n",
            "1  ...    0.0  0.258518     0.0  hazard_yearly_flood.csv      NaN NaN NaN NaN   \n",
            "2  ...    0.0  0.322190     0.0  hazard_yearly_flood.csv      NaN NaN NaN NaN   \n",
            "3  ...    0.0  0.183475     0.0  hazard_yearly_flood.csv      NaN NaN NaN NaN   \n",
            "4  ...    0.0  0.266701     0.0  hazard_yearly_flood.csv      NaN NaN NaN NaN   \n",
            "\n",
            "   CPRI_index  CPRI_norm  \n",
            "0         NaN        NaN  \n",
            "1         NaN        NaN  \n",
            "2         NaN        NaN  \n",
            "3         NaN        NaN  \n",
            "4         NaN        NaN  \n",
            "\n",
            "[5 rows x 21 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# --- F√°jlok el√©r√©si √∫tja ---\n",
        "ASSETS_XLSX = \"/content/assets.xlsx\"\n",
        "COMBINED_CSV = \"/content/combined_cpri_data.csv\"\n",
        "OUTPUT_PATH = \"/content/cpri_outputs\"\n",
        "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
        "\n",
        "# --- 1Ô∏è‚É£ Beolvas√°s ---\n",
        "assets = pd.read_excel(ASSETS_XLSX)\n",
        "assets.rename(columns={'latitude': 'Latitude', 'longitude': 'Longitude'}, inplace=True)\n",
        "assets['country'] = assets['country'].str.strip().str.lower()\n",
        "\n",
        "data = pd.read_csv(COMBINED_CSV)\n",
        "\n",
        "print(f\"Beolvasott assetek: {assets.shape[0]}, adatsorok: {data.shape[0]}\")\n",
        "\n",
        "# --- 2Ô∏è‚É£ Orsz√°gn√©v-lek√©pez√©s a hazard r√©gi√≥kra ---\n",
        "region_map = {\n",
        "    \"denmark\": \"dania\",\n",
        "    \"germany\": \"nemet\",\n",
        "    \"netherlands\": \"uk\",   # holland adatok az UK-f√°jlban voltak\n",
        "    \"uk\": \"uk\",\n",
        "    \"united kingdom\": \"uk\",\n",
        "    \"taiwan\": \"tajvan\",\n",
        "    \"usa\": \"usa\",\n",
        "    \"united states\": \"usa\",\n",
        "    \"us\": \"usa\"\n",
        "}\n",
        "\n",
        "# --- 3Ô∏è‚É£ Sz≈±r√©s csak a hazard adatokra ---\n",
        "hazard_df = data[data['source'].str.contains(\"hazard_yearly\")].copy()\n",
        "hazard_df = hazard_df[['hazard','region','year','H_norm','P_norm']].dropna(subset=['H_norm','P_norm'])\n",
        "print(\"Hazard-r√©gi√≥k:\", hazard_df['region'].unique())\n",
        "\n",
        "# --- 4Ô∏è‚É£ √öj r_values t√°bl√°zat l√©trehoz√°sa ---\n",
        "results = []\n",
        "for _, a in assets.iterrows():\n",
        "    country = a['country']\n",
        "    region = region_map.get(country, country)\n",
        "    subset = hazard_df[hazard_df['region'].str.lower() == region]\n",
        "    if subset.empty:\n",
        "        continue\n",
        "\n",
        "    for _, s in subset.iterrows():\n",
        "        r = s['H_norm'] * s['P_norm'] * 1.0 * 0.5\n",
        "        results.append({\n",
        "            'asset_id': a['asset_id'],\n",
        "            'hazard': s['hazard'],\n",
        "            'year': int(s['year']),\n",
        "            'H_norm': s['H_norm'],\n",
        "            'P_norm': s['P_norm'],\n",
        "            'E': 1.0,\n",
        "            'V': 0.5,\n",
        "            'r': r\n",
        "        })\n",
        "\n",
        "r_df = pd.DataFrame(results)\n",
        "r_df.to_csv(f\"{OUTPUT_PATH}/r_values_rebuilt.csv\", index=False)\n",
        "print(f\"‚úÖ √öj r_values_rebuilt.csv mentve ({len(r_df)} sor)\")\n",
        "\n",
        "# --- 5Ô∏è‚É£ CPRI index √∫jrasz√°m√≠t√°sa ---\n",
        "agg = r_df.groupby(['asset_id','hazard','year'], as_index=False)['r'].mean()\n",
        "cpri = agg.groupby(['asset_id','year'], as_index=False)['r'].mean()\n",
        "cpri.rename(columns={'r': 'CPRI_index'}, inplace=True)\n",
        "\n",
        "# normaliz√°l√°s √©v szerint\n",
        "cpri['CPRI_norm'] = cpri.groupby('year')['CPRI_index'].transform(\n",
        "    lambda x: (x - x.min()) / (x.max() - x.min())\n",
        ")\n",
        "\n",
        "out_path = f\"{OUTPUT_PATH}/cpri_index_rebuilt.csv\"\n",
        "cpri.to_csv(out_path, index=False)\n",
        "print(f\"‚úÖ √öj cpri_index_rebuilt.csv mentve ({len(cpri)} sor)\")\n",
        "print(cpri.head(10))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MCwjIWOIJcTa",
        "outputId": "a56ced06-50ef-4ab4-cbc6-1ea1e0f7d51d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Beolvasott assetek: 20, adatsorok: 1320\n",
            "Hazard-r√©gi√≥k: ['uk' 'nemet' 'usa' 'tajvan' 'dania']\n",
            "‚úÖ √öj r_values_rebuilt.csv mentve (1438 sor)\n",
            "‚úÖ √öj cpri_index_rebuilt.csv mentve (480 sor)\n",
            "  asset_id  year  CPRI_index  CPRI_norm\n",
            "0   DE-001  2000    0.136033    1.00000\n",
            "1   DE-001  2001    0.096679    1.00000\n",
            "2   DE-001  2002    0.144593    1.00000\n",
            "3   DE-001  2003    0.099303    1.00000\n",
            "4   DE-001  2004    0.111654    1.00000\n",
            "5   DE-001  2005    0.158265    1.00000\n",
            "6   DE-001  2006    0.109066    0.38377\n",
            "7   DE-001  2007    0.146028    1.00000\n",
            "8   DE-001  2008    0.141180    1.00000\n",
            "9   DE-001  2009    0.148049    0.83310\n"
          ]
        }
      ]
    }
  ]
}